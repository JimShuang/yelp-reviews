{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Yelp Dataset\n",
    "[**The Yelp Dataset**](https://www.yelp.com/dataset_challenge/) is a dataset published by the business review service [Yelp](http://yelp.com) for academic research and educational purposes. I really like the Yelp dataset as a subject for machine learning and natural language processing demos, because it's big (but not so big that you need your own data center to process it), well-connected, and anyone can relate to it &mdash; it's largely about food, after all!\n",
    "\n",
    "**Note:** If you'd like to execute this notebook interactively on your local machine, you'll need to download your own copy of the Yelp dataset. If you're reviewing a static copy of the notebook online, you can skip this step. Here's how to get the dataset:\n",
    "1. Please visit the Yelp dataset webpage [here](https://www.yelp.com/dataset_challenge/)\n",
    "1. Click \"Get the Data\"\n",
    "1. Please review, agree to, and respect Yelp's terms of use!\n",
    "1. The dataset downloads as a compressed .tgz file; uncompress it\n",
    "1. Place the uncompressed dataset files (*yelp_academic_dataset_business.json*, etc.) in a directory named *yelp_dataset_challenge_academic_dataset*\n",
    "1. Place the *yelp_dataset_challenge_academic_dataset* within the *data* directory in the *Modern NLP in Python* project folder\n",
    "\n",
    "That's it! You're ready to go.\n",
    "\n",
    "The files are text files (UTF-8) with one _json object_ per line, each one corresponding to an individual data record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !conda install -y tensorflow\n",
    "# !conda install -y keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import codecs\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 42\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review records are stored in a similar manner &mdash; _key, value_ pairs containing information about the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"VfBHSwC5Vz_pbFluy07i9Q\",\"user_id\":\"cjpdDjZyprfyDG3RlkVG3w\",\"business_id\":\"uYHaNptLzDLoV_JZ_MuzUA\",\"stars\":5,\"date\":\"2016-07-12\",\"text\":\"My girlfriend and I stayed here for 3 nights and loved it. The location of this hotel and very decent price makes this an amazing deal. When you walk out the front door Scott Monument and Princes street are right in front of you, Edinburgh Castle and the Royal Mile is a 2 minute walk via a close right around the corner, and there are so many hidden gems nearby including Calton Hill and the newly opened Arches that made this location incredible.\\n\\nThe hotel itself was also very nice with a reasonably priced bar, very considerate staff, and small but comfortable rooms with excellent bathrooms and showers. Only two minor complaints are no telephones in room for room service (not a huge deal for us) and no AC in the room, but they have huge windows which can be fully opened. The staff were incredible though, letting us borrow umbrellas for the rain, giving us maps and directions, and also when we had lost our only UK adapter for charging our phones gave us a very fancy one for free.\\n\\nI would highly recommend this hotel to friends, and when I return to Edinburgh (which I most definitely will) I will be staying here without any hesitation.\",\"useful\":0,\"funny\":0,\"cool\":0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_dir = os.path.join('..', 'data',\n",
    "                              'yelp_dataset_challenge_academic_dataset', 'dataset')\n",
    "\n",
    "json_review_filepath = os.path.join(json_dir,\n",
    "                                    'review.json')\n",
    "\n",
    "with open(json_review_filepath, encoding='utf_8') as f:\n",
    "    first_review_record = f.readline()\n",
    "    \n",
    "print(first_review_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few attributes of note on the review records:\n",
    "- __text__ &mdash; _the natural language text the user wrote_\n",
    "- __stars__ &mdash; _the number of stars the reviewer left_\n",
    "\n",
    "The _text_ and the _stars_ attribute will be our focus today!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_data_dir = os.path.join('..','data','sentiment_data')\n",
    "\n",
    "text_filepath = os.path.join(sentiment_data_dir,'sentiment.txt')\n",
    "sentiment_filepath = os.path.join(sentiment_data_dir,'number_of_stars.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 4739863 reviews in the txt file.\n",
      "CPU times: user 9.08 s, sys: 1.63 s, total: 10.7 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make the if statement True\n",
    "# if you want to execute data prep yourself once you've got the yelp dataset saved.\n",
    "\n",
    "if False:\n",
    "    \n",
    "    review_count = 0\n",
    "\n",
    "    # create & open a new files in write mode\n",
    "    with open(text_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "        with open(sentiment_filepath, 'w', encoding='utf_8') as review_sentiment_file:\n",
    "\n",
    "            # open the existing review json file\n",
    "            with open(json_review_filepath, encoding='utf_8') as review_json_file:\n",
    "                # loop through all reviews in the existing file and convert to dict\n",
    "                for review_json in review_json_file:\n",
    "                    review = json.loads(review_json)\n",
    "                    # write the review as a line in the new file\n",
    "                    # escape newline characters in the original review text\n",
    "                    review_txt_file.write(review.get('text','NA').replace('\\n', r'\\n') + '\\n')\n",
    "                    review_sentiment_file.write(str(review.get('stars','NA')) +'\\n')\n",
    "                    review_count =  review_count + 1\n",
    "\n",
    "    print ('Text from {} reviews written to the new txt file.'.format(review_count))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with open(text_filepath, encoding='utf_8') as review_txt_file:\n",
    "        for review_count, line in enumerate(review_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print('Text from {} reviews in the txt file.'.format(review_count + 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #count the lines in the above files\n",
    "\n",
    "# from itertools import (takewhile,repeat)\n",
    "\n",
    "# def rawincount(filename):\n",
    "#     with open(filename, 'rb') as f:\n",
    "#         bufgen = takewhile(lambda x: x, (f.raw.read(1024*1024) for _ in repeat(None)))\n",
    "#         return sum( buf.count(b'\\n') for buf in bufgen )\n",
    "\n",
    "# print('Len of review text file:{}\\nLen of review sentiment file:{}'.format(rawincount(review_txt_filepath), rawincount(review_sentiment_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Good!  The lengths of the files match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets do train-test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_train_dir = os.path.join(sentiment_data_dir, 'train')\n",
    "sentiment_test_dir = os.path.join(sentiment_data_dir, 'test')\n",
    "\n",
    "X_train_file_path = os.path.join(sentiment_train_dir, 'X_train.txt')\n",
    "y_train_file_path = os.path.join(sentiment_train_dir, 'y_train.txt')\n",
    "X_test_file_path = os.path.join(sentiment_test_dir, 'X_test.txt')\n",
    "y_test_file_path = os.path.join(sentiment_test_dir, 'y_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_sentiment(line): \n",
    "    ''' convert sentiment text (1-5 star rating) into positive or negative review'''\n",
    "    try:\n",
    "        if int(line.rstrip()) >= 3: #three stars or higher is positive review\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        pass\n",
    "    return \"NA\"\n",
    "\n",
    "#Build subset of X and y in memory\n",
    "num_rows = 30000\n",
    "with open(sentiment_filepath, 'r') as f:\n",
    "    y = [find_sentiment(line) for num,line in enumerate(f) if num < num_rows ]\n",
    "\n",
    "with open(text_filepath, 'r') as f:\n",
    "    X = [line.rstrip() for num,line in enumerate(f) if num < num_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 6104, 1: 23896})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the sentiment ratings\n",
    "count = Counter(y)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=SEED)\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 19136, 0: 4864}) Counter({1: 19136, 0: 4864})\n"
     ]
    }
   ],
   "source": [
    "y_train_counter = Counter(y_train)\n",
    "y_test_counter = Counter(y_test)\n",
    "\n",
    "print(y_train_counter, y_train_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata ...............\n",
      "Solving package specifications: .\n",
      "\n",
      "Package plan for installation in environment /anaconda:\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    pandas: 0.20.3-py36_0 --> 0.21.0-py36_0 conda-forge\n",
      "\n",
      "pandas-0.21.0- 100% |################################| Time: 0:00:03   3.08 MB/s\n"
     ]
    }
   ],
   "source": [
    "!conda install -y pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata ...............\n",
      "Solving package specifications: .\n",
      "\n",
      "Package plan for installation in environment /anaconda:\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    dask-core:        0.15.4-py_0   conda-forge\n",
      "    distributed:      1.19.3-py36_0 conda-forge\n",
      "    msgpack-python:   0.4.8-py36_0  conda-forge\n",
      "    sortedcontainers: 1.5.7-py36_0  conda-forge\n",
      "    tblib:            1.3.2-py36_0  conda-forge\n",
      "    zict:             0.1.3-py_0    conda-forge\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    bokeh:            0.12.5-py36_0             --> 0.12.10-py36_0 conda-forge\n",
      "    dask:             0.13.0-py36_0             --> 0.15.4-py_0    conda-forge\n",
      "    partd:            0.3.7-py36_0              --> 0.3.8-py36_0   conda-forge\n",
      "    tornado:          4.4.2-py36_0              --> 4.5.2-py36_0   conda-forge\n",
      "\n",
      "dask-core-0.15 100% |################################| Time: 0:00:00   1.92 MB/s\n",
      "msgpack-python 100% |################################| Time: 0:00:00   1.91 MB/s\n",
      "sortedcontaine 100% |################################| Time: 0:00:00   1.06 MB/s\n",
      "tblib-1.3.2-py 100% |################################| Time: 0:00:00   7.16 MB/s\n",
      "tornado-4.5.2- 100% |################################| Time: 0:00:00   2.78 MB/s\n",
      "partd-0.3.8-py 100% |################################| Time: 0:00:00  12.30 MB/s\n",
      "zict-0.1.3-py_ 100% |################################| Time: 0:00:00   9.50 MB/s\n",
      "bokeh-0.12.10- 100% |################################| Time: 0:00:00   6.26 MB/s\n",
      "distributed-1. 100% |################################| Time: 0:00:00  11.75 MB/s\n",
      "dask-0.15.4-py 100% |################################| Time: 0:00:00   2.82 MB/s\n"
     ]
    }
   ],
   "source": [
    "!conda update -y dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.6'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    #print string\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \"\", string)\n",
    "    string = re.sub(r\"\\)\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \"\", string)\n",
    "    string = re.sub(r\"/\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "    return string.strip().lower()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
