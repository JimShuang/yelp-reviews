{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Yelp Dataset\n",
    "[**The Yelp Dataset**](https://www.yelp.com/dataset_challenge/) is a dataset published by the business review service [Yelp](http://yelp.com) for academic research and educational purposes. I really like the Yelp dataset as a subject for machine learning and natural language processing demos, because it's big (but not so big that you need your own data center to process it), well-connected, and anyone can relate to it &mdash; it's largely about food, after all!\n",
    "\n",
    "**Note:** If you'd like to execute this notebook interactively on your local machine, you'll need to download your own copy of the Yelp dataset. If you're reviewing a static copy of the notebook online, you can skip this step. Here's how to get the dataset:\n",
    "1. Please visit the Yelp dataset webpage [here](https://www.yelp.com/dataset_challenge/)\n",
    "1. Click \"Get the Data\"\n",
    "1. Please review, agree to, and respect Yelp's terms of use!\n",
    "1. The dataset downloads as a compressed .tgz file; uncompress it\n",
    "1. Place the uncompressed dataset files (*yelp_academic_dataset_business.json*, etc.) in a directory named *yelp_dataset_challenge_academic_dataset*\n",
    "1. Place the *yelp_dataset_challenge_academic_dataset* within the *data* directory in the *Modern NLP in Python* project folder\n",
    "\n",
    "That's it! You're ready to go.\n",
    "\n",
    "The files are text files (UTF-8) with one _json object_ per line, each one corresponding to an individual data record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "# !conda install -y tensorflow\n",
    "# !conda install -y keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.layers import Dense, Embedding, Input, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#!conda update -y dask #if pandas version is above approx. 0.19 Keras throws exception without dask update.\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review records are stored in a similar manner &mdash; _key, value_ pairs containing information about the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few attributes of note on the review records:\n",
    "- __text__ &mdash; _the natural language text the user wrote_\n",
    "- __stars__ &mdash; _the number of stars the reviewer left_\n",
    "\n",
    "The _text_ and the _stars_ attribute will be our focus today!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"VfBHSwC5Vz_pbFluy07i9Q\",\"user_id\":\"cjpdDjZyprfyDG3RlkVG3w\",\"business_id\":\"uYHaNptLzDLoV_JZ_MuzUA\",\"stars\":5,\"date\":\"2016-07-12\",\"text\":\"My girlfriend and I stayed here for 3 nights and loved it. The location of this hotel and very decent price makes this an amazing deal. When you walk out the front door Scott Monument and Princes street are right in front of you, Edinburgh Castle and the Royal Mile is a 2 minute walk via a close right around the corner, and there are so many hidden gems nearby including Calton Hill and the newly opened Arches that made this location incredible.\\n\\nThe hotel itself was also very nice with a reasonably priced bar, very considerate staff, and small but comfortable rooms with excellent bathrooms and showers. Only two minor complaints are no telephones in room for room service (not a huge deal for us) and no AC in the room, but they have huge windows which can be fully opened. The staff were incredible though, letting us borrow umbrellas for the rain, giving us maps and directions, and also when we had lost our only UK adapter for charging our phones gave us a very fancy one for free.\\n\\nI would highly recommend this hotel to friends, and when I return to Edinburgh (which I most definitely will) I will be staying here without any hesitation.\",\"useful\":0,\"funny\":0,\"cool\":0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_dir = os.path.join('..', 'data',\n",
    "                              'yelp_dataset_challenge_academic_dataset', 'dataset')\n",
    "\n",
    "json_review_filepath = os.path.join(json_dir,\n",
    "                                    'review.json')\n",
    "\n",
    "with open(json_review_filepath, encoding='utf_8') as f:\n",
    "    first_review_record = f.readline()\n",
    "    \n",
    "print(first_review_record)\n",
    "\n",
    "sentiment_data_dir = os.path.join('..','data','sentiment_data')\n",
    "\n",
    "text_filepath = os.path.join(sentiment_data_dir,'sentiment.txt')\n",
    "sentiment_filepath = os.path.join(sentiment_data_dir,'number_of_stars.txt')\n",
    "\n",
    "sentiment_train_dir = os.path.join(sentiment_data_dir, 'train')\n",
    "sentiment_test_dir = os.path.join(sentiment_data_dir, 'test')\n",
    "\n",
    "X_train_file_path = os.path.join(sentiment_train_dir, 'X_train.txt')\n",
    "y_train_file_path = os.path.join(sentiment_train_dir, 'y_train.txt')\n",
    "X_test_file_path = os.path.join(sentiment_test_dir, 'X_test.txt')\n",
    "y_test_file_path = os.path.join(sentiment_test_dir, 'y_test.txt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of review text file:4736897\n",
      "Len of review sentiment file:4736897\n",
      "CPU times: user 2.46 s, sys: 1.43 s, total: 3.89 s\n",
      "Wall time: 4.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make the if statement True\n",
    "# if you want to execute data prep yourself once you've got the yelp dataset saved.\n",
    "\n",
    "data_dir = os.path.join('..','data')\n",
    "stopword_en_filepath = os.path.join(data_dir, 'stopwords-en.txt')\n",
    "\n",
    "\n",
    "if False:\n",
    "    #load stopwords\n",
    "    with open(stopword_en_filepath) as f:\n",
    "        stopwords_en = set(f.read().split('\\n'))\n",
    "    \n",
    "    review_count = 0\n",
    "\n",
    "    # create & open a new files in write mode\n",
    "    with open(text_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "        with open(sentiment_filepath, 'w', encoding='utf_8') as review_sentiment_file:\n",
    "\n",
    "            # open the existing review json file\n",
    "            with open(json_review_filepath, encoding='utf_8') as review_json_file:\n",
    "                # loop through all reviews in the existing file and convert to dict\n",
    "                for review_json in review_json_file:\n",
    "                    review = json.loads(review_json)\n",
    "                    # write the review as a line in the new file\n",
    "                    # escape newline characters in the original review text\n",
    "                    \n",
    "                    #TTD     pull stopwords out before writing file\n",
    "                    #TTD     make sure review is english, ie does not have german or french or spanish stopwords\n",
    "                    review_txt_file.write(review.get('text','NA').replace('\\n', r'\\n') + '\\n')\n",
    "                    review_sentiment_file.write(str(review.get('stars','NA')) +'\\n')\n",
    "                    review_count =  review_count + 1\n",
    "\n",
    "    print ('Text from {} reviews written to the new txt file.'.format(review_count))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    #count the lines in the above files\n",
    "\n",
    "    from itertools import (takewhile,repeat)\n",
    "\n",
    "    def rawincount(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            bufgen = takewhile(lambda x: x, (f.raw.read(1024*1024) for _ in repeat(None)))\n",
    "            return sum( buf.count(b'\\n') for buf in bufgen )\n",
    "\n",
    "    print('Len of review text file:{}\\nLen of review sentiment file:{}'.format(rawincount(text_filepath), \n",
    "                                                                               rawincount(sentiment_filepath)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "NUM_ROWS = 5000 #number of rows to load into memory\n",
    "\n",
    "\n",
    "\n",
    "def find_sentiment(line): \n",
    "    ''' convert sentiment text (1-5 star rating) into positive or negative review'''\n",
    "    if int(line.rstrip()) >= 3: #three stars or higher is positive review\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "with open(sentiment_filepath, 'r') as f:\n",
    "    y = [find_sentiment(line) for rows,line in enumerate(f) if rows < NUM_ROWS ]\n",
    "\n",
    "with open(text_filepath, 'r') as f:\n",
    "    X = [line.rstrip() for rows,line in enumerate(f) if rows < NUM_ROWS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20227 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#next, vectorize the text samples into a 2D integer tensor based on words\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split of positive and neg sentiment of y: Counter({1: 3929, 0: 1071})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "count = Counter(y)\n",
    "print('split of positive and neg sentiment of y:',count)\n",
    "\n",
    "def median(lst):\n",
    "    quotient, remainder = divmod(len(lst), 2)\n",
    "    if remainder:\n",
    "        return sorted(lst)[quotient]\n",
    "    return sum(sorted(lst)[quotient - 1:quotient + 1]) / 2\n",
    "#check distribution of lengths of reviews\n",
    "len_x = [len(item) for item in X]\n",
    "max_length_count = max(len_x)\n",
    "median_length_count = median(len_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 1459, 2, 4, 1572, 38, 11, 154, 1332, 2, 346, 8, 1, 178, 7, 18, 549, 2, 35, 371, 201, 384, 18, 59, 159, 479, 52, 17, 372, 37, 1, 426, 624, 10812, 10813, 2, 3665, 524, 27, 164, 10, 426, 7, 17, 1611, 97, 2, 1, 3666, 2347, 9, 3, 123, 831, 372, 2774, 3, 462, 164, 176, 1, 1098, 2, 40, 27, 25, 214, 1356, 4608, 1161, 678, 10814, 2270, 2, 1, 4203, 606, 8046, 14, 134, 18, 178, 915, 12, 78, 549, 574, 6, 72, 35, 83, 16, 3, 1273, 650, 226, 35, 5098, 127, 2, 126, 15, 775, 1384, 16, 275, 2013, 2, 10815, 69, 144, 2775, 1854, 27, 68, 10816, 10, 329, 11, 329, 51, 22, 3, 388, 479, 11, 95, 2, 68, 6661, 10, 1, 329, 15, 19, 23, 388, 2271, 63, 65, 32, 2435, 606, 1, 127, 26, 915, 187, 2776, 95, 8047, 5718, 11, 1, 3236, 739, 95, 6662, 2, 4204, 2, 72, 52, 21, 24, 1306, 56, 69, 5719, 8048, 11, 2777, 56, 3926, 330, 95, 3, 35, 836, 46, 11, 293, 12, 98, 58, 391, 185, 18, 549, 5, 263, 2, 52, 4, 525, 5, 1611, 63, 4, 211, 121, 67, 4, 67, 32, 1540, 38, 308, 160, 4205]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n",
    "# # first, build index mapping words in the embeddings set\n",
    "# # to their embedding vector\n",
    "\n",
    "# print('Indexing word vectors.')\n",
    "\n",
    "glove_dir = os.path.join('..','data', 'glove_data') #location of the word embeddings\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (4000, 1000)\n",
      "X_test shape: (1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words+1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            mask_zero=True,\n",
    "                            trainable=False)\n",
    "\n",
    "## TRANSFORM Training input to sequences\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Training...\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "  80/4000 [..............................] - ETA: 1530s - loss: 0.5176 - acc: 0.7875\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    }
   ],
   "source": [
    "model_dir = os.path.join('models', 'yelp_sentiment_analysis_glove_embeddings')\n",
    "model_json_filepath = os.path.join(model_dir,\"model.json\" )\n",
    "model_weights_filepath = os.path.join(model_dir, 'model.h5')\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "model = None\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "if False:\n",
    "    try:\n",
    "        # load model and weights from disk\n",
    "        with open(model_json_filepath, 'r') as json_file:\n",
    "            model_json = json_file.read()\n",
    "            model = model_from_json(model_json)\n",
    "        # load weights into new model\n",
    "        model.load_weights(model_weights_filepath)\n",
    "        print(\"Loaded model from disk!\")\n",
    "    except Exception as e:\n",
    "        print('Could not load file from disk', e)\n",
    "\n",
    "if model is None:\n",
    "    print('Building model...')\n",
    "    model = build_model()\n",
    "    \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=1,\n",
    "          callbacks=[EarlyStopping(patience=3, verbose=1)],\n",
    "          validation_data=(X_test, y_test)\n",
    "         )\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=BATCH_SIZE)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "\n",
    "with open(model_json_filepath, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(model_weights_filepath)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
