{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What you can learn about food by analyzing a million Yelp reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Yelp Dataset\n",
    "[**The Yelp Dataset**](https://www.yelp.com/dataset_challenge/) is a dataset published by the business review service [Yelp](http://yelp.com) for academic research and educational purposes. I really like the Yelp dataset as a subject for machine learning and natural language processing demos, because it's big (but not so big that you need your own data center to process it), well-connected, and anyone can relate to it &mdash; it's largely about food, after all!\n",
    "\n",
    "**Note:** If you'd like to execute this notebook interactively on your local machine, you'll need to download your own copy of the Yelp dataset. If you're reviewing a static copy of the notebook online, you can skip this step. Here's how to get the dataset:\n",
    "1. Please visit the Yelp dataset webpage [here](https://www.yelp.com/dataset_challenge/)\n",
    "1. Click \"Get the Data\"\n",
    "1. Please review, agree to, and respect Yelp's terms of use!\n",
    "1. The dataset downloads as a compressed .tgz file; uncompress it\n",
    "1. Place the uncompressed dataset files (*yelp_academic_dataset_business.json*, etc.) in a directory named *yelp_dataset_challenge_academic_dataset*\n",
    "1. Place the *yelp_dataset_challenge_academic_dataset* within the *data* directory in the *Modern NLP in Python* project folder\n",
    "\n",
    "That's it! You're ready to go.\n",
    "\n",
    "The current iteration of the Yelp dataset (as of this demo) consists of the following data:\n",
    "- __552K__ users\n",
    "- __77K__ businesses\n",
    "- __2.2M__ user reviews\n",
    "\n",
    "When focusing on restaurants alone, there are approximately __22K__ restaurants with approximately __1M__ user reviews written about them.\n",
    "\n",
    "The data is provided in a handful of files in _.json_ format. We'll be using the following files for our demo:\n",
    "- __yelp\\_academic\\_dataset\\_business.json__ &mdash; _the records for individual businesses_\n",
    "- __yelp\\_academic\\_dataset\\_review.json__ &mdash; _the records for reviews users wrote about businesses_\n",
    "\n",
    "The files are text files (UTF-8) with one _json object_ per line, each one corresponding to an individual data record. Let's take a look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "data_directory = os.path.join('..', 'data',\n",
    "                              'yelp_dataset_challenge_academic_dataset', 'dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review records are stored in a similar manner &mdash; _key, value_ pairs containing information about the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"VfBHSwC5Vz_pbFluy07i9Q\",\"user_id\":\"cjpdDjZyprfyDG3RlkVG3w\",\"business_id\":\"uYHaNptLzDLoV_JZ_MuzUA\",\"stars\":5,\"date\":\"2016-07-12\",\"text\":\"My girlfriend and I stayed here for 3 nights and loved it. The location of this hotel and very decent price makes this an amazing deal. When you walk out the front door Scott Monument and Princes street are right in front of you, Edinburgh Castle and the Royal Mile is a 2 minute walk via a close right around the corner, and there are so many hidden gems nearby including Calton Hill and the newly opened Arches that made this location incredible.\\n\\nThe hotel itself was also very nice with a reasonably priced bar, very considerate staff, and small but comfortable rooms with excellent bathrooms and showers. Only two minor complaints are no telephones in room for room service (not a huge deal for us) and no AC in the room, but they have huge windows which can be fully opened. The staff were incredible though, letting us borrow umbrellas for the rain, giving us maps and directions, and also when we had lost our only UK adapter for charging our phones gave us a very fancy one for free.\\n\\nI would highly recommend this hotel to friends, and when I return to Edinburgh (which I most definitely will) I will be staying here without any hesitation.\",\"useful\":0,\"funny\":0,\"cool\":0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_filepath = os.path.join(data_directory,\n",
    "                                    'review.json')\n",
    "\n",
    "with open(review_filepath, encoding='utf_8') as f:\n",
    "    first_review_record = f.readline()\n",
    "    \n",
    "print(first_review_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few attributes of note on the review records:\n",
    "- __text__ &mdash; _the natural language text the user wrote_\n",
    "- __stars__ &mdash; _the number of stars the reviewer left_\n",
    "\n",
    "The _text_ and the _stars_ attribute will be our focus today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a new directory that contains only the text from reviews about restaurants, with one review per line in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join('..','data','yelp_dataset_challenge_academic_dataset', 'extracted_from_json')\n",
    "\n",
    "review_txt_filepath = os.path.join(intermediate_directory,'sentiment_data',\n",
    "                                   'review_text_for_sentiment.txt')\n",
    "\n",
    "review_sentiment_filepath = os.path.join(intermediate_directory, 'sentiment_data','sentiment_of_review_text.txt')\n",
    "\n",
    "review_json_filepath = os.path.join(data_directory,'review.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 4736897 reviews written to the new txt file.\n",
      "CPU times: user 1min 7s, sys: 18 s, total: 1min 25s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "\n",
    "if True:\n",
    "    \n",
    "    review_count = 0\n",
    "\n",
    "    # create & open a new files in write mode\n",
    "    with open(review_txt_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "        with open(review_sentiment_filepath, 'w', encoding='utf_8') as review_sentiment_file:\n",
    "\n",
    "            # open the existing review json file\n",
    "            with open(review_json_filepath, encoding='utf_8') as review_json_file:\n",
    "                # loop through all reviews in the existing file and convert to dict\n",
    "                for review_json in review_json_file:\n",
    "                    review = json.loads(review_json)\n",
    "                    # write the review as a line in the new file\n",
    "                    # escape newline characters in the original review text\n",
    "                    review_txt_file.write(review.get('text','NA').replace('\\n', '\\\\n') + '\\n')\n",
    "                    review_sentiment_file.write(str(review.get('stars','NA')) +'\\n')\n",
    "                    review_count =  review_count + 1\n",
    "\n",
    "    print ('Text from {} reviews written to the new txt file.'.format(review_count))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with open(review_txt_filepath, encoding='utf_8') as review_txt_file:\n",
    "        for review_count, line in enumerate(review_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print('Text from {} reviews in the txt file.'.format(review_count + 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of review text file:4736897\n",
      "Len of review sentiment file:4736897\n"
     ]
    }
   ],
   "source": [
    "#count the lines in the above files\n",
    "\n",
    "from itertools import (takewhile,repeat)\n",
    "\n",
    "def rawincount(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        bufgen = takewhile(lambda x: x, (f.raw.read(1024*1024) for _ in repeat(None)))\n",
    "        return sum( buf.count(b'\\n') for buf in bufgen )\n",
    "\n",
    "print('Len of review text file:{}\\nLen of review sentiment file:{}'.format(rawincount(review_txt_filepath), rawincount(review_sentiment_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Good!  The lengths of the files match!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy &mdash; Industrial-Strength NLP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spaCy](https://s3.amazonaws.com/skipgram-images/spaCy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**spaCy**](https://spacy.io) is an industrial-strength natural language processing (_NLP_) library for Python. spaCy's goal is to take recent advancements in natural language processing out of research papers and put them in the hands of users to build production software.\n",
    "\n",
    "spaCy handles many tasks commonly associated with building an end-to-end natural language processing pipeline:\n",
    "- Tokenization\n",
    "- Text normalization, such as lowercasing, stemming/lemmatization\n",
    "- Part-of-speech tagging\n",
    "- Syntactic dependency parsing\n",
    "- Sentence boundary detection\n",
    "- Named entity recognition and annotation\n",
    "\n",
    "In the \"batteries included\" Python tradition, spaCy contains built-in data and models which you can use out-of-the-box for processing general-purpose English language text:\n",
    "- Large English vocabulary, including stopword lists\n",
    "- Token \"probabilities\"\n",
    "- Word vectors\n",
    "\n",
    "spaCy is written in optimized Cython, which means it's _fast_. According to a few independent sources, it's the fastest syntactic parser available in any language. Key pieces of the spaCy parsing pipeline are written in pure C, enabling efficient multithreading (i.e., spaCy can release the _GIL_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[93m    Link en_default already exists\u001b[0m\r\n",
      "\r\n",
      "    To overwrite an existing link, use the --force flag.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_md\n",
    "#!python -m spacy link en_core_web_md en_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "nlp = spacy.load('en_default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab a sample review to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time at this group of hotels. Pretty new, only one in UK, another to open in Edinburgh and one in London. Rooms not very big but great price and location for a weekend in Edinburgh. Rooms clean, comfortable, good shower and free wifi!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(review_txt_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 8, 9))[0]\n",
    "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
    "        \n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector Embedding with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop quiz! Can you complete this text snippet?\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word2vec quiz](https://s3.amazonaws.com/skipgram-images/word2vec-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "You just demonstrated the core machine learning concept behind word vector embedding models!\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word2vec quiz 2](https://s3.amazonaws.com/skipgram-images/word2vec-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of *word vector embedding models*, or *word vector models* for short, is to learn dense, numerical vector representations for each term in a corpus vocabulary. If the model is successful, the vectors it learns about each term should encode some information about the *meaning* or *concept* the term represents, and the relationship between it and other terms in the vocabulary. Word vector models are also fully unsupervised &mdash; they learn all of these meanings and relationships solely by analyzing the text of the corpus, without any advance knowledge provided.\n",
    "\n",
    "Perhaps the best-known word vector model is [word2vec](https://arxiv.org/pdf/1301.3781v3.pdf), originally proposed in 2013. The general idea of word2vec is, for a given *focus word*, to use the *context* of the word &mdash; i.e., the other words immediately before and after it &mdash; to provide hints about what the focus word might mean. To do this, word2vec uses a *sliding window* technique, where it considers snippets of text only a few tokens long at a time.\n",
    "\n",
    "At the start of the learning process, the model initializes random vectors for all terms in the corpus vocabulary. The model then slides the window across every snippet of text in the corpus, with each word taking turns as the focus word. Each time the model considers a new snippet, it tries to learn some information about the focus word based on the surrouding context, and it \"nudges\" the words' vector representations accordingly. One complete pass sliding the window across all of the corpus text is known as a training *epoch*. It's common to train a word2vec model for multiple passes/epochs over the corpus. Over time, the model rearranges the terms' vector representations such that terms that frequently appear in similar contexts have vector representations that are *close* to each other in vector space.\n",
    "\n",
    "For a deeper dive into word2vec's machine learning process, see [here](https://arxiv.org/pdf/1411.2738v4.pdf).\n",
    "\n",
    "Word2vec has a number of user-defined hyperparameters, including:\n",
    "- The dimensionality of the vectors. Typical choices include a few dozen to several hundred.\n",
    "- The width of the sliding window, in tokens. Five is a common default choice, but narrower and wider windows are possible.\n",
    "- The number of training epochs.\n",
    "\n",
    "For using word2vec in Python, [gensim](https://rare-technologies.com/deep-learning-with-word2vec-and-gensim/) comes to the rescue again! It offers a [highly-optimized](https://rare-technologies.com/word2vec-in-python-part-two-optimizing/), [parallelized](https://rare-technologies.com/parallelizing-word2vec-in-python/) implementation of the word2vec algorithm with its [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our word2vec model using the normalized sentences with our phrase models applied. We'll use 100-dimensional vectors, and set up our training process to run for twelve epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 training epochs so far.\n",
      "CPU times: user 5.43 s, sys: 891 ms, total: 6.32 s\n",
      "Wall time: 7.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the word2vec model yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    # initiate the model and perform the first epoch of training\n",
    "    food2vec = Word2Vec(trigram_sentences, size=100, window=5,\n",
    "                        min_count=20, sg=1, workers=4)\n",
    "    \n",
    "    food2vec.save(word2vec_filepath)\n",
    "\n",
    "    # perform another 11 epochs of training\n",
    "    for i in range(1,12):\n",
    "\n",
    "        food2vec.train(trigram_sentences)\n",
    "        food2vec.save(word2vec_filepath)\n",
    "        \n",
    "# load the finished model from disk\n",
    "food2vec = Word2Vec.load(word2vec_filepath)\n",
    "food2vec.init_sims()\n",
    "\n",
    "print u'{} training epochs so far.'.format(food2vec.train_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my four-core machine, each epoch over all the text in the ~1 million Yelp reviews takes about 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50,835 terms in the food2vec vocabulary.\n"
     ]
    }
   ],
   "source": [
    "print u'{:,} terms in the food2vec vocabulary.'.format(len(food2vec.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the word vectors our model has learned. We'll create a pandas DataFrame with the terms as the row labels, and the 100 dimensions of the word vector model as the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.035762</td>\n",
       "      <td>-0.173890</td>\n",
       "      <td>-0.035782</td>\n",
       "      <td>-0.007144</td>\n",
       "      <td>0.032371</td>\n",
       "      <td>-0.065272</td>\n",
       "      <td>-0.219383</td>\n",
       "      <td>-0.064665</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.025802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050136</td>\n",
       "      <td>0.044030</td>\n",
       "      <td>0.145281</td>\n",
       "      <td>-0.020442</td>\n",
       "      <td>0.128879</td>\n",
       "      <td>-0.076461</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>-0.012841</td>\n",
       "      <td>0.024710</td>\n",
       "      <td>-0.067555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>-0.074780</td>\n",
       "      <td>-0.049524</td>\n",
       "      <td>0.085974</td>\n",
       "      <td>-0.098892</td>\n",
       "      <td>0.141556</td>\n",
       "      <td>0.024878</td>\n",
       "      <td>-0.011119</td>\n",
       "      <td>-0.175374</td>\n",
       "      <td>0.005410</td>\n",
       "      <td>-0.110996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.199047</td>\n",
       "      <td>-0.081284</td>\n",
       "      <td>-0.198344</td>\n",
       "      <td>0.007257</td>\n",
       "      <td>0.075339</td>\n",
       "      <td>0.070266</td>\n",
       "      <td>-0.008326</td>\n",
       "      <td>-0.127542</td>\n",
       "      <td>-0.046246</td>\n",
       "      <td>0.110279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.070505</td>\n",
       "      <td>-0.026918</td>\n",
       "      <td>0.028344</td>\n",
       "      <td>-0.099909</td>\n",
       "      <td>0.127974</td>\n",
       "      <td>-0.058155</td>\n",
       "      <td>-0.056091</td>\n",
       "      <td>-0.028973</td>\n",
       "      <td>0.197281</td>\n",
       "      <td>-0.040528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049051</td>\n",
       "      <td>-0.212434</td>\n",
       "      <td>-0.042576</td>\n",
       "      <td>0.055731</td>\n",
       "      <td>0.117097</td>\n",
       "      <td>-0.206737</td>\n",
       "      <td>0.055435</td>\n",
       "      <td>-0.065056</td>\n",
       "      <td>0.052316</td>\n",
       "      <td>-0.078666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>-0.161238</td>\n",
       "      <td>0.050831</td>\n",
       "      <td>-0.081706</td>\n",
       "      <td>-0.084479</td>\n",
       "      <td>0.053073</td>\n",
       "      <td>-0.102327</td>\n",
       "      <td>-0.108607</td>\n",
       "      <td>-0.001920</td>\n",
       "      <td>-0.057367</td>\n",
       "      <td>-0.050715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028528</td>\n",
       "      <td>-0.016578</td>\n",
       "      <td>-0.179229</td>\n",
       "      <td>0.053357</td>\n",
       "      <td>0.070913</td>\n",
       "      <td>0.036893</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>-0.007254</td>\n",
       "      <td>-0.056005</td>\n",
       "      <td>0.106345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.083491</td>\n",
       "      <td>-0.033712</td>\n",
       "      <td>-0.124125</td>\n",
       "      <td>-0.110776</td>\n",
       "      <td>-0.033046</td>\n",
       "      <td>-0.089950</td>\n",
       "      <td>0.025416</td>\n",
       "      <td>-0.052321</td>\n",
       "      <td>-0.059281</td>\n",
       "      <td>0.074985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101939</td>\n",
       "      <td>0.022392</td>\n",
       "      <td>0.057049</td>\n",
       "      <td>0.015819</td>\n",
       "      <td>-0.001798</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>0.037175</td>\n",
       "      <td>-0.074279</td>\n",
       "      <td>0.001683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.012082</td>\n",
       "      <td>0.033135</td>\n",
       "      <td>-0.063183</td>\n",
       "      <td>-0.057252</td>\n",
       "      <td>-0.018721</td>\n",
       "      <td>-0.017931</td>\n",
       "      <td>-0.027784</td>\n",
       "      <td>0.112110</td>\n",
       "      <td>0.020549</td>\n",
       "      <td>-0.174336</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017111</td>\n",
       "      <td>-0.067532</td>\n",
       "      <td>-0.022149</td>\n",
       "      <td>0.154788</td>\n",
       "      <td>-0.093789</td>\n",
       "      <td>-0.020456</td>\n",
       "      <td>0.065478</td>\n",
       "      <td>0.075484</td>\n",
       "      <td>-0.053530</td>\n",
       "      <td>-0.005314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.025022</td>\n",
       "      <td>0.081581</td>\n",
       "      <td>0.127987</td>\n",
       "      <td>-0.188015</td>\n",
       "      <td>0.041450</td>\n",
       "      <td>-0.126222</td>\n",
       "      <td>0.172725</td>\n",
       "      <td>-0.149931</td>\n",
       "      <td>-0.069566</td>\n",
       "      <td>-0.036031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045720</td>\n",
       "      <td>0.094828</td>\n",
       "      <td>0.089329</td>\n",
       "      <td>0.051623</td>\n",
       "      <td>-0.108989</td>\n",
       "      <td>-0.145476</td>\n",
       "      <td>0.068617</td>\n",
       "      <td>0.090687</td>\n",
       "      <td>-0.101725</td>\n",
       "      <td>0.090377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>-0.140812</td>\n",
       "      <td>-0.070552</td>\n",
       "      <td>0.022102</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.109890</td>\n",
       "      <td>-0.061365</td>\n",
       "      <td>0.046450</td>\n",
       "      <td>0.003073</td>\n",
       "      <td>0.113845</td>\n",
       "      <td>-0.038957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051071</td>\n",
       "      <td>-0.090922</td>\n",
       "      <td>-0.022011</td>\n",
       "      <td>0.157082</td>\n",
       "      <td>-0.082406</td>\n",
       "      <td>-0.010306</td>\n",
       "      <td>-0.063481</td>\n",
       "      <td>-0.098728</td>\n",
       "      <td>-0.064020</td>\n",
       "      <td>0.153466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.036341</td>\n",
       "      <td>-0.054903</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>-0.010602</td>\n",
       "      <td>0.168195</td>\n",
       "      <td>-0.058505</td>\n",
       "      <td>-0.052342</td>\n",
       "      <td>0.039159</td>\n",
       "      <td>-0.053572</td>\n",
       "      <td>-0.160039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085908</td>\n",
       "      <td>-0.211464</td>\n",
       "      <td>-0.084990</td>\n",
       "      <td>0.082315</td>\n",
       "      <td>0.223018</td>\n",
       "      <td>-0.142501</td>\n",
       "      <td>0.280647</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>-0.037710</td>\n",
       "      <td>-0.145140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>-0.075276</td>\n",
       "      <td>0.109047</td>\n",
       "      <td>0.055135</td>\n",
       "      <td>0.052251</td>\n",
       "      <td>0.209437</td>\n",
       "      <td>0.084334</td>\n",
       "      <td>-0.122419</td>\n",
       "      <td>-0.193307</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>-0.099067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150619</td>\n",
       "      <td>-0.060446</td>\n",
       "      <td>0.181940</td>\n",
       "      <td>-0.118538</td>\n",
       "      <td>-0.002879</td>\n",
       "      <td>0.018827</td>\n",
       "      <td>0.084586</td>\n",
       "      <td>0.040437</td>\n",
       "      <td>0.070277</td>\n",
       "      <td>-0.047521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>-0.102976</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>-0.069402</td>\n",
       "      <td>-0.122936</td>\n",
       "      <td>0.028278</td>\n",
       "      <td>-0.074256</td>\n",
       "      <td>-0.013786</td>\n",
       "      <td>-0.147065</td>\n",
       "      <td>0.204125</td>\n",
       "      <td>-0.033473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032123</td>\n",
       "      <td>0.013365</td>\n",
       "      <td>0.008156</td>\n",
       "      <td>-0.021331</td>\n",
       "      <td>0.025385</td>\n",
       "      <td>0.105075</td>\n",
       "      <td>0.184737</td>\n",
       "      <td>0.087325</td>\n",
       "      <td>-0.230621</td>\n",
       "      <td>0.075051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>-0.053390</td>\n",
       "      <td>-0.175599</td>\n",
       "      <td>-0.091688</td>\n",
       "      <td>-0.153791</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>0.013146</td>\n",
       "      <td>-0.013261</td>\n",
       "      <td>0.162506</td>\n",
       "      <td>-0.036985</td>\n",
       "      <td>-0.123813</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101997</td>\n",
       "      <td>-0.025117</td>\n",
       "      <td>0.101147</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>-0.075434</td>\n",
       "      <td>-0.031021</td>\n",
       "      <td>0.170358</td>\n",
       "      <td>-0.070997</td>\n",
       "      <td>-0.143472</td>\n",
       "      <td>-0.039543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.015929</td>\n",
       "      <td>-0.019187</td>\n",
       "      <td>-0.186680</td>\n",
       "      <td>-0.240963</td>\n",
       "      <td>0.077926</td>\n",
       "      <td>-0.122313</td>\n",
       "      <td>-0.183584</td>\n",
       "      <td>-0.038707</td>\n",
       "      <td>0.067121</td>\n",
       "      <td>-0.108626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054799</td>\n",
       "      <td>-0.029601</td>\n",
       "      <td>-0.197221</td>\n",
       "      <td>-0.081994</td>\n",
       "      <td>0.114129</td>\n",
       "      <td>0.127746</td>\n",
       "      <td>0.057743</td>\n",
       "      <td>-0.044793</td>\n",
       "      <td>-0.080014</td>\n",
       "      <td>-0.001816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>-0.026609</td>\n",
       "      <td>0.085940</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.011576</td>\n",
       "      <td>0.156952</td>\n",
       "      <td>-0.061402</td>\n",
       "      <td>-0.068207</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>-0.169472</td>\n",
       "      <td>0.051105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088909</td>\n",
       "      <td>0.062827</td>\n",
       "      <td>-0.114507</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>-0.075059</td>\n",
       "      <td>-0.202200</td>\n",
       "      <td>0.003658</td>\n",
       "      <td>0.042448</td>\n",
       "      <td>-0.091925</td>\n",
       "      <td>0.045213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>-0.063436</td>\n",
       "      <td>0.089140</td>\n",
       "      <td>-0.057425</td>\n",
       "      <td>-0.093110</td>\n",
       "      <td>0.066531</td>\n",
       "      <td>-0.079715</td>\n",
       "      <td>-0.049745</td>\n",
       "      <td>-0.161346</td>\n",
       "      <td>0.097094</td>\n",
       "      <td>0.035439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129163</td>\n",
       "      <td>-0.022460</td>\n",
       "      <td>-0.200731</td>\n",
       "      <td>0.079950</td>\n",
       "      <td>-0.002590</td>\n",
       "      <td>-0.113734</td>\n",
       "      <td>0.048470</td>\n",
       "      <td>0.037333</td>\n",
       "      <td>0.111525</td>\n",
       "      <td>-0.001558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>0.042318</td>\n",
       "      <td>-0.186670</td>\n",
       "      <td>-0.230563</td>\n",
       "      <td>0.076302</td>\n",
       "      <td>0.216593</td>\n",
       "      <td>-0.056183</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>-0.087819</td>\n",
       "      <td>0.073513</td>\n",
       "      <td>-0.219137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036514</td>\n",
       "      <td>0.135176</td>\n",
       "      <td>-0.056771</td>\n",
       "      <td>-0.020261</td>\n",
       "      <td>0.213735</td>\n",
       "      <td>-0.116074</td>\n",
       "      <td>0.162992</td>\n",
       "      <td>0.015298</td>\n",
       "      <td>-0.152731</td>\n",
       "      <td>0.070306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>-0.002067</td>\n",
       "      <td>-0.023159</td>\n",
       "      <td>0.035879</td>\n",
       "      <td>0.036316</td>\n",
       "      <td>-0.110738</td>\n",
       "      <td>-0.033034</td>\n",
       "      <td>-0.100291</td>\n",
       "      <td>-0.039403</td>\n",
       "      <td>0.109342</td>\n",
       "      <td>0.024952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051417</td>\n",
       "      <td>0.220378</td>\n",
       "      <td>-0.106171</td>\n",
       "      <td>0.159718</td>\n",
       "      <td>-0.036391</td>\n",
       "      <td>-0.025573</td>\n",
       "      <td>0.133651</td>\n",
       "      <td>-0.157615</td>\n",
       "      <td>0.010161</td>\n",
       "      <td>-0.172925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.139739</td>\n",
       "      <td>0.184600</td>\n",
       "      <td>0.137359</td>\n",
       "      <td>-0.109916</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>-0.018423</td>\n",
       "      <td>-0.027546</td>\n",
       "      <td>-0.055886</td>\n",
       "      <td>-0.137625</td>\n",
       "      <td>-0.058589</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035842</td>\n",
       "      <td>-0.075413</td>\n",
       "      <td>-0.068598</td>\n",
       "      <td>0.122231</td>\n",
       "      <td>-0.097841</td>\n",
       "      <td>0.114074</td>\n",
       "      <td>0.111075</td>\n",
       "      <td>0.174843</td>\n",
       "      <td>-0.018743</td>\n",
       "      <td>0.087721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>-0.171262</td>\n",
       "      <td>-0.119866</td>\n",
       "      <td>0.063801</td>\n",
       "      <td>-0.087287</td>\n",
       "      <td>-0.061923</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>-0.196524</td>\n",
       "      <td>-0.043654</td>\n",
       "      <td>-0.003327</td>\n",
       "      <td>-0.078496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094615</td>\n",
       "      <td>0.029243</td>\n",
       "      <td>0.020553</td>\n",
       "      <td>-0.101657</td>\n",
       "      <td>0.039655</td>\n",
       "      <td>0.059782</td>\n",
       "      <td>-0.073931</td>\n",
       "      <td>-0.002060</td>\n",
       "      <td>-0.068405</td>\n",
       "      <td>-0.246893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>0.057481</td>\n",
       "      <td>0.044937</td>\n",
       "      <td>-0.063766</td>\n",
       "      <td>-0.007839</td>\n",
       "      <td>0.161119</td>\n",
       "      <td>-0.047322</td>\n",
       "      <td>-0.024250</td>\n",
       "      <td>-0.038904</td>\n",
       "      <td>0.085989</td>\n",
       "      <td>0.036280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084352</td>\n",
       "      <td>-0.119525</td>\n",
       "      <td>0.076835</td>\n",
       "      <td>-0.010369</td>\n",
       "      <td>0.035561</td>\n",
       "      <td>0.055588</td>\n",
       "      <td>0.119598</td>\n",
       "      <td>0.306402</td>\n",
       "      <td>-0.095085</td>\n",
       "      <td>0.053575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>-0.235321</td>\n",
       "      <td>-0.026314</td>\n",
       "      <td>0.143165</td>\n",
       "      <td>-0.170460</td>\n",
       "      <td>0.042189</td>\n",
       "      <td>-0.019444</td>\n",
       "      <td>-0.171945</td>\n",
       "      <td>-0.087666</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>-0.034397</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122975</td>\n",
       "      <td>-0.054745</td>\n",
       "      <td>0.022250</td>\n",
       "      <td>-0.068428</td>\n",
       "      <td>-0.009932</td>\n",
       "      <td>-0.012489</td>\n",
       "      <td>0.102740</td>\n",
       "      <td>0.071282</td>\n",
       "      <td>-0.165166</td>\n",
       "      <td>0.126805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>-0.164133</td>\n",
       "      <td>0.007745</td>\n",
       "      <td>0.058311</td>\n",
       "      <td>-0.169839</td>\n",
       "      <td>-0.042278</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.203732</td>\n",
       "      <td>-0.021252</td>\n",
       "      <td>-0.084491</td>\n",
       "      <td>-0.016372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096869</td>\n",
       "      <td>0.060159</td>\n",
       "      <td>-0.133541</td>\n",
       "      <td>0.166804</td>\n",
       "      <td>0.084901</td>\n",
       "      <td>0.109261</td>\n",
       "      <td>0.137871</td>\n",
       "      <td>0.018093</td>\n",
       "      <td>-0.158754</td>\n",
       "      <td>-0.042917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>0.091428</td>\n",
       "      <td>-0.132115</td>\n",
       "      <td>0.105080</td>\n",
       "      <td>0.135949</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>-0.046825</td>\n",
       "      <td>-0.165575</td>\n",
       "      <td>-0.087334</td>\n",
       "      <td>0.068053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101949</td>\n",
       "      <td>-0.037880</td>\n",
       "      <td>-0.187836</td>\n",
       "      <td>0.037602</td>\n",
       "      <td>-0.094156</td>\n",
       "      <td>-0.040069</td>\n",
       "      <td>-0.013014</td>\n",
       "      <td>-0.013038</td>\n",
       "      <td>-0.033346</td>\n",
       "      <td>-0.056112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>-0.239592</td>\n",
       "      <td>-0.232940</td>\n",
       "      <td>-0.005036</td>\n",
       "      <td>-0.028226</td>\n",
       "      <td>0.149816</td>\n",
       "      <td>-0.133312</td>\n",
       "      <td>-0.034164</td>\n",
       "      <td>-0.130310</td>\n",
       "      <td>-0.013757</td>\n",
       "      <td>0.008618</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034984</td>\n",
       "      <td>-0.135347</td>\n",
       "      <td>-0.112965</td>\n",
       "      <td>0.056312</td>\n",
       "      <td>0.055106</td>\n",
       "      <td>-0.026181</td>\n",
       "      <td>-0.135510</td>\n",
       "      <td>0.087664</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>-0.111619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>0.025479</td>\n",
       "      <td>0.130311</td>\n",
       "      <td>0.119834</td>\n",
       "      <td>-0.096365</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.074431</td>\n",
       "      <td>-0.063780</td>\n",
       "      <td>0.063191</td>\n",
       "      <td>-0.004273</td>\n",
       "      <td>0.111458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076251</td>\n",
       "      <td>-0.076574</td>\n",
       "      <td>-0.086146</td>\n",
       "      <td>-0.023936</td>\n",
       "      <td>0.136419</td>\n",
       "      <td>-0.001543</td>\n",
       "      <td>-0.084301</td>\n",
       "      <td>0.016356</td>\n",
       "      <td>-0.148379</td>\n",
       "      <td>-0.016498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>0.021455</td>\n",
       "      <td>0.079794</td>\n",
       "      <td>0.192058</td>\n",
       "      <td>-0.093809</td>\n",
       "      <td>-0.094279</td>\n",
       "      <td>-0.147522</td>\n",
       "      <td>-0.066564</td>\n",
       "      <td>-0.073133</td>\n",
       "      <td>0.009708</td>\n",
       "      <td>0.050529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050627</td>\n",
       "      <td>-0.008651</td>\n",
       "      <td>-0.034267</td>\n",
       "      <td>0.045445</td>\n",
       "      <td>-0.104442</td>\n",
       "      <td>-0.012076</td>\n",
       "      <td>-0.118052</td>\n",
       "      <td>-0.015163</td>\n",
       "      <td>-0.006679</td>\n",
       "      <td>-0.074553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>0.009313</td>\n",
       "      <td>-0.101684</td>\n",
       "      <td>-0.163864</td>\n",
       "      <td>-0.159002</td>\n",
       "      <td>0.018936</td>\n",
       "      <td>-0.056202</td>\n",
       "      <td>-0.074619</td>\n",
       "      <td>-0.127081</td>\n",
       "      <td>0.182303</td>\n",
       "      <td>-0.001993</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040294</td>\n",
       "      <td>-0.038149</td>\n",
       "      <td>-0.180993</td>\n",
       "      <td>-0.143341</td>\n",
       "      <td>0.140279</td>\n",
       "      <td>0.181399</td>\n",
       "      <td>0.054530</td>\n",
       "      <td>-0.152596</td>\n",
       "      <td>0.028443</td>\n",
       "      <td>-0.030319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>0.031094</td>\n",
       "      <td>-0.126839</td>\n",
       "      <td>-0.054429</td>\n",
       "      <td>-0.221885</td>\n",
       "      <td>-0.063464</td>\n",
       "      <td>0.024554</td>\n",
       "      <td>0.060154</td>\n",
       "      <td>-0.011108</td>\n",
       "      <td>-0.020744</td>\n",
       "      <td>0.038979</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074458</td>\n",
       "      <td>-0.172092</td>\n",
       "      <td>-0.123518</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>-0.085149</td>\n",
       "      <td>0.157569</td>\n",
       "      <td>-0.048633</td>\n",
       "      <td>0.017931</td>\n",
       "      <td>0.111066</td>\n",
       "      <td>0.040107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>0.102501</td>\n",
       "      <td>-0.095756</td>\n",
       "      <td>-0.216304</td>\n",
       "      <td>-0.107230</td>\n",
       "      <td>-0.112544</td>\n",
       "      <td>-0.036979</td>\n",
       "      <td>-0.066605</td>\n",
       "      <td>-0.016080</td>\n",
       "      <td>0.046475</td>\n",
       "      <td>-0.128300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058233</td>\n",
       "      <td>-0.046821</td>\n",
       "      <td>0.042406</td>\n",
       "      <td>0.178607</td>\n",
       "      <td>0.181424</td>\n",
       "      <td>-0.120113</td>\n",
       "      <td>0.029031</td>\n",
       "      <td>0.113648</td>\n",
       "      <td>-0.107441</td>\n",
       "      <td>-0.005374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>0.141091</td>\n",
       "      <td>0.073669</td>\n",
       "      <td>0.109637</td>\n",
       "      <td>-0.112564</td>\n",
       "      <td>-0.167600</td>\n",
       "      <td>-0.059139</td>\n",
       "      <td>-0.122552</td>\n",
       "      <td>-0.137383</td>\n",
       "      <td>0.093218</td>\n",
       "      <td>0.096284</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052156</td>\n",
       "      <td>-0.106116</td>\n",
       "      <td>-0.088926</td>\n",
       "      <td>-0.079129</td>\n",
       "      <td>0.072921</td>\n",
       "      <td>-0.009605</td>\n",
       "      <td>-0.001447</td>\n",
       "      <td>0.068642</td>\n",
       "      <td>-0.022845</td>\n",
       "      <td>0.197407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard_boiled_egg</th>\n",
       "      <td>0.025154</td>\n",
       "      <td>0.060949</td>\n",
       "      <td>-0.064816</td>\n",
       "      <td>0.071975</td>\n",
       "      <td>0.087870</td>\n",
       "      <td>-0.034552</td>\n",
       "      <td>0.046470</td>\n",
       "      <td>-0.074013</td>\n",
       "      <td>0.048614</td>\n",
       "      <td>-0.027098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161784</td>\n",
       "      <td>-0.011800</td>\n",
       "      <td>0.036288</td>\n",
       "      <td>0.102444</td>\n",
       "      <td>-0.036660</td>\n",
       "      <td>-0.259496</td>\n",
       "      <td>0.093633</td>\n",
       "      <td>-0.055128</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.069598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poached_quail_egg</th>\n",
       "      <td>0.060616</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>-0.052908</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>0.031732</td>\n",
       "      <td>-0.020164</td>\n",
       "      <td>0.067209</td>\n",
       "      <td>0.015895</td>\n",
       "      <td>0.034758</td>\n",
       "      <td>-0.247553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074396</td>\n",
       "      <td>0.051672</td>\n",
       "      <td>0.038299</td>\n",
       "      <td>-0.001248</td>\n",
       "      <td>0.074726</td>\n",
       "      <td>-0.081252</td>\n",
       "      <td>-0.078778</td>\n",
       "      <td>0.008109</td>\n",
       "      <td>0.023891</td>\n",
       "      <td>0.075863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>egg_foo</th>\n",
       "      <td>0.022816</td>\n",
       "      <td>0.077288</td>\n",
       "      <td>-0.122898</td>\n",
       "      <td>-0.022765</td>\n",
       "      <td>-0.137979</td>\n",
       "      <td>-0.131516</td>\n",
       "      <td>-0.025750</td>\n",
       "      <td>0.071284</td>\n",
       "      <td>-0.130913</td>\n",
       "      <td>-0.041167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044335</td>\n",
       "      <td>0.100112</td>\n",
       "      <td>0.124033</td>\n",
       "      <td>-0.012426</td>\n",
       "      <td>0.037161</td>\n",
       "      <td>0.058809</td>\n",
       "      <td>-0.033203</td>\n",
       "      <td>-0.105034</td>\n",
       "      <td>0.207285</td>\n",
       "      <td>-0.094321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sherri</th>\n",
       "      <td>-0.089536</td>\n",
       "      <td>-0.030370</td>\n",
       "      <td>0.011311</td>\n",
       "      <td>-0.094766</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>0.143681</td>\n",
       "      <td>-0.035314</td>\n",
       "      <td>-0.060650</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>-0.034962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010114</td>\n",
       "      <td>0.059706</td>\n",
       "      <td>0.068094</td>\n",
       "      <td>0.158161</td>\n",
       "      <td>-0.076122</td>\n",
       "      <td>0.115804</td>\n",
       "      <td>-0.133826</td>\n",
       "      <td>0.022022</td>\n",
       "      <td>-0.115043</td>\n",
       "      <td>0.043500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hindrance</th>\n",
       "      <td>-0.033325</td>\n",
       "      <td>0.011520</td>\n",
       "      <td>0.027370</td>\n",
       "      <td>0.240223</td>\n",
       "      <td>-0.004098</td>\n",
       "      <td>-0.047842</td>\n",
       "      <td>-0.008730</td>\n",
       "      <td>-0.086408</td>\n",
       "      <td>-0.155469</td>\n",
       "      <td>0.036887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028251</td>\n",
       "      <td>0.136807</td>\n",
       "      <td>0.079745</td>\n",
       "      <td>-0.107322</td>\n",
       "      <td>-0.092331</td>\n",
       "      <td>0.151693</td>\n",
       "      <td>0.126414</td>\n",
       "      <td>-0.036642</td>\n",
       "      <td>0.043212</td>\n",
       "      <td>-0.016060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggdrop_soup</th>\n",
       "      <td>-0.028369</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>-0.005378</td>\n",
       "      <td>-0.088736</td>\n",
       "      <td>0.015203</td>\n",
       "      <td>-0.150734</td>\n",
       "      <td>-0.008286</td>\n",
       "      <td>0.052836</td>\n",
       "      <td>0.041498</td>\n",
       "      <td>0.061759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058192</td>\n",
       "      <td>0.170658</td>\n",
       "      <td>-0.069061</td>\n",
       "      <td>-0.043249</td>\n",
       "      <td>0.080981</td>\n",
       "      <td>-0.074859</td>\n",
       "      <td>0.026031</td>\n",
       "      <td>-0.048331</td>\n",
       "      <td>0.196082</td>\n",
       "      <td>0.050968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arbitrarily</th>\n",
       "      <td>0.203714</td>\n",
       "      <td>-0.047405</td>\n",
       "      <td>-0.045261</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>-0.074105</td>\n",
       "      <td>-0.011420</td>\n",
       "      <td>0.006737</td>\n",
       "      <td>0.068212</td>\n",
       "      <td>-0.010306</td>\n",
       "      <td>0.162682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165282</td>\n",
       "      <td>-0.078750</td>\n",
       "      <td>-0.051527</td>\n",
       "      <td>0.129190</td>\n",
       "      <td>-0.088332</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.021954</td>\n",
       "      <td>0.224878</td>\n",
       "      <td>-0.020637</td>\n",
       "      <td>0.019025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faisant</th>\n",
       "      <td>0.046053</td>\n",
       "      <td>0.083873</td>\n",
       "      <td>0.057943</td>\n",
       "      <td>0.174203</td>\n",
       "      <td>-0.121259</td>\n",
       "      <td>-0.043806</td>\n",
       "      <td>-0.069513</td>\n",
       "      <td>-0.037047</td>\n",
       "      <td>-0.026478</td>\n",
       "      <td>-0.119066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063223</td>\n",
       "      <td>0.018042</td>\n",
       "      <td>-0.107165</td>\n",
       "      <td>0.028304</td>\n",
       "      <td>-0.141706</td>\n",
       "      <td>-0.084532</td>\n",
       "      <td>0.097593</td>\n",
       "      <td>-0.029115</td>\n",
       "      <td>-0.016920</td>\n",
       "      <td>-0.027668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marian</th>\n",
       "      <td>-0.035330</td>\n",
       "      <td>0.146843</td>\n",
       "      <td>-0.173594</td>\n",
       "      <td>-0.010971</td>\n",
       "      <td>-0.150150</td>\n",
       "      <td>0.082224</td>\n",
       "      <td>0.036275</td>\n",
       "      <td>-0.028033</td>\n",
       "      <td>-0.076082</td>\n",
       "      <td>0.051976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042506</td>\n",
       "      <td>0.160951</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.168254</td>\n",
       "      <td>-0.057206</td>\n",
       "      <td>-0.067292</td>\n",
       "      <td>-0.254453</td>\n",
       "      <td>0.049995</td>\n",
       "      <td>-0.097059</td>\n",
       "      <td>0.106862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9:30p</th>\n",
       "      <td>0.119010</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.102635</td>\n",
       "      <td>-0.016918</td>\n",
       "      <td>-0.075822</td>\n",
       "      <td>-0.016008</td>\n",
       "      <td>0.012505</td>\n",
       "      <td>-0.068003</td>\n",
       "      <td>-0.066301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027208</td>\n",
       "      <td>-0.110685</td>\n",
       "      <td>0.139527</td>\n",
       "      <td>-0.031720</td>\n",
       "      <td>-0.101919</td>\n",
       "      <td>0.105827</td>\n",
       "      <td>0.034134</td>\n",
       "      <td>0.083859</td>\n",
       "      <td>0.089299</td>\n",
       "      <td>0.053762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extra_chasu</th>\n",
       "      <td>-0.225889</td>\n",
       "      <td>-0.131615</td>\n",
       "      <td>0.046431</td>\n",
       "      <td>0.017999</td>\n",
       "      <td>0.119188</td>\n",
       "      <td>-0.075226</td>\n",
       "      <td>-0.140749</td>\n",
       "      <td>-0.054829</td>\n",
       "      <td>0.210201</td>\n",
       "      <td>-0.098395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049070</td>\n",
       "      <td>-0.028876</td>\n",
       "      <td>-0.173717</td>\n",
       "      <td>0.074353</td>\n",
       "      <td>-0.078363</td>\n",
       "      <td>-0.166292</td>\n",
       "      <td>-0.007546</td>\n",
       "      <td>0.116509</td>\n",
       "      <td>0.073052</td>\n",
       "      <td>0.090262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lavosh_wrap</th>\n",
       "      <td>0.134420</td>\n",
       "      <td>-0.032055</td>\n",
       "      <td>0.012240</td>\n",
       "      <td>0.024420</td>\n",
       "      <td>0.031334</td>\n",
       "      <td>-0.002414</td>\n",
       "      <td>0.022550</td>\n",
       "      <td>-0.024545</td>\n",
       "      <td>0.054083</td>\n",
       "      <td>0.101219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092470</td>\n",
       "      <td>-0.109932</td>\n",
       "      <td>0.019652</td>\n",
       "      <td>-0.090741</td>\n",
       "      <td>-0.008825</td>\n",
       "      <td>0.052382</td>\n",
       "      <td>0.012688</td>\n",
       "      <td>-0.035351</td>\n",
       "      <td>-0.093695</td>\n",
       "      <td>-0.105152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dum_dum</th>\n",
       "      <td>-0.002741</td>\n",
       "      <td>-0.137371</td>\n",
       "      <td>0.030704</td>\n",
       "      <td>-0.030365</td>\n",
       "      <td>-0.134645</td>\n",
       "      <td>-0.036521</td>\n",
       "      <td>-0.019889</td>\n",
       "      <td>-0.191169</td>\n",
       "      <td>0.034061</td>\n",
       "      <td>0.156001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072851</td>\n",
       "      <td>0.016341</td>\n",
       "      <td>-0.009848</td>\n",
       "      <td>0.038048</td>\n",
       "      <td>-0.026917</td>\n",
       "      <td>-0.035949</td>\n",
       "      <td>-0.022561</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>0.026049</td>\n",
       "      <td>0.074344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triplet</th>\n",
       "      <td>-0.010495</td>\n",
       "      <td>0.057432</td>\n",
       "      <td>-0.019535</td>\n",
       "      <td>-0.044881</td>\n",
       "      <td>0.042409</td>\n",
       "      <td>-0.094355</td>\n",
       "      <td>0.111214</td>\n",
       "      <td>-0.141414</td>\n",
       "      <td>-0.102281</td>\n",
       "      <td>0.013674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>-0.071423</td>\n",
       "      <td>0.029983</td>\n",
       "      <td>-0.079734</td>\n",
       "      <td>0.017278</td>\n",
       "      <td>0.049596</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>-0.111090</td>\n",
       "      <td>0.125764</td>\n",
       "      <td>-0.020409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nantucket</th>\n",
       "      <td>-0.124356</td>\n",
       "      <td>0.141918</td>\n",
       "      <td>-0.038579</td>\n",
       "      <td>0.035650</td>\n",
       "      <td>-0.157662</td>\n",
       "      <td>0.048110</td>\n",
       "      <td>-0.006915</td>\n",
       "      <td>0.049056</td>\n",
       "      <td>0.191926</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076790</td>\n",
       "      <td>-0.067047</td>\n",
       "      <td>0.020261</td>\n",
       "      <td>0.088759</td>\n",
       "      <td>0.029744</td>\n",
       "      <td>0.020393</td>\n",
       "      <td>-0.033682</td>\n",
       "      <td>0.150856</td>\n",
       "      <td>0.276557</td>\n",
       "      <td>-0.086213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gurl</th>\n",
       "      <td>-0.119794</td>\n",
       "      <td>0.025898</td>\n",
       "      <td>-0.070130</td>\n",
       "      <td>-0.027929</td>\n",
       "      <td>0.113244</td>\n",
       "      <td>0.076868</td>\n",
       "      <td>0.084859</td>\n",
       "      <td>-0.000508</td>\n",
       "      <td>-0.008275</td>\n",
       "      <td>0.026478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038612</td>\n",
       "      <td>0.010746</td>\n",
       "      <td>0.052930</td>\n",
       "      <td>0.236658</td>\n",
       "      <td>0.021199</td>\n",
       "      <td>-0.092340</td>\n",
       "      <td>-0.143270</td>\n",
       "      <td>0.038394</td>\n",
       "      <td>0.005431</td>\n",
       "      <td>0.177151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nordstroms</th>\n",
       "      <td>-0.030410</td>\n",
       "      <td>-0.026861</td>\n",
       "      <td>-0.016836</td>\n",
       "      <td>0.097363</td>\n",
       "      <td>-0.098189</td>\n",
       "      <td>0.080675</td>\n",
       "      <td>0.005855</td>\n",
       "      <td>0.065331</td>\n",
       "      <td>-0.047586</td>\n",
       "      <td>-0.083942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029618</td>\n",
       "      <td>-0.259797</td>\n",
       "      <td>0.078861</td>\n",
       "      <td>0.081747</td>\n",
       "      <td>-0.147309</td>\n",
       "      <td>0.084849</td>\n",
       "      <td>-0.121320</td>\n",
       "      <td>0.219587</td>\n",
       "      <td>-0.045757</td>\n",
       "      <td>-0.032065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eau_de</th>\n",
       "      <td>-0.146450</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>-0.094391</td>\n",
       "      <td>0.146833</td>\n",
       "      <td>-0.051392</td>\n",
       "      <td>0.026519</td>\n",
       "      <td>0.022764</td>\n",
       "      <td>-0.161101</td>\n",
       "      <td>-0.106951</td>\n",
       "      <td>0.018758</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051030</td>\n",
       "      <td>-0.074207</td>\n",
       "      <td>-0.017950</td>\n",
       "      <td>-0.091572</td>\n",
       "      <td>-0.142917</td>\n",
       "      <td>-0.220402</td>\n",
       "      <td>0.008315</td>\n",
       "      <td>-0.086124</td>\n",
       "      <td>0.101774</td>\n",
       "      <td>0.051059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extra_.5</th>\n",
       "      <td>-0.124972</td>\n",
       "      <td>-0.032596</td>\n",
       "      <td>-0.017800</td>\n",
       "      <td>0.106415</td>\n",
       "      <td>-0.086728</td>\n",
       "      <td>-0.039636</td>\n",
       "      <td>-0.048088</td>\n",
       "      <td>-0.016923</td>\n",
       "      <td>-0.079315</td>\n",
       "      <td>0.078559</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180088</td>\n",
       "      <td>0.053834</td>\n",
       "      <td>-0.146145</td>\n",
       "      <td>0.114461</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.033901</td>\n",
       "      <td>0.040767</td>\n",
       "      <td>0.131177</td>\n",
       "      <td>0.154335</td>\n",
       "      <td>0.170376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hazelnut_crunch</th>\n",
       "      <td>0.055723</td>\n",
       "      <td>-0.098708</td>\n",
       "      <td>0.013225</td>\n",
       "      <td>0.098075</td>\n",
       "      <td>-0.021967</td>\n",
       "      <td>-0.046137</td>\n",
       "      <td>0.083640</td>\n",
       "      <td>0.011891</td>\n",
       "      <td>-0.034513</td>\n",
       "      <td>0.159220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098174</td>\n",
       "      <td>0.030037</td>\n",
       "      <td>-0.028975</td>\n",
       "      <td>0.043462</td>\n",
       "      <td>0.046888</td>\n",
       "      <td>-0.187656</td>\n",
       "      <td>0.048226</td>\n",
       "      <td>-0.086743</td>\n",
       "      <td>0.060892</td>\n",
       "      <td>0.115703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bella_notte</th>\n",
       "      <td>0.209296</td>\n",
       "      <td>-0.102068</td>\n",
       "      <td>-0.059274</td>\n",
       "      <td>-0.061223</td>\n",
       "      <td>0.032078</td>\n",
       "      <td>0.042433</td>\n",
       "      <td>0.060573</td>\n",
       "      <td>-0.263855</td>\n",
       "      <td>0.026291</td>\n",
       "      <td>-0.082852</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089840</td>\n",
       "      <td>-0.048790</td>\n",
       "      <td>-0.040089</td>\n",
       "      <td>-0.112727</td>\n",
       "      <td>-0.008889</td>\n",
       "      <td>-0.095856</td>\n",
       "      <td>-0.047674</td>\n",
       "      <td>0.194045</td>\n",
       "      <td>-0.118062</td>\n",
       "      <td>0.139320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homebrew</th>\n",
       "      <td>0.054390</td>\n",
       "      <td>-0.050279</td>\n",
       "      <td>-0.181006</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>-0.064048</td>\n",
       "      <td>0.029383</td>\n",
       "      <td>0.007203</td>\n",
       "      <td>-0.207681</td>\n",
       "      <td>-0.026785</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138264</td>\n",
       "      <td>-0.097649</td>\n",
       "      <td>0.139629</td>\n",
       "      <td>0.101149</td>\n",
       "      <td>-0.145391</td>\n",
       "      <td>0.143334</td>\n",
       "      <td>0.121490</td>\n",
       "      <td>-0.018492</td>\n",
       "      <td>0.134825</td>\n",
       "      <td>0.120686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conveyor_belt_oven</th>\n",
       "      <td>-0.099600</td>\n",
       "      <td>-0.127618</td>\n",
       "      <td>0.070998</td>\n",
       "      <td>-0.040632</td>\n",
       "      <td>-0.022066</td>\n",
       "      <td>-0.021832</td>\n",
       "      <td>-0.066087</td>\n",
       "      <td>-0.130704</td>\n",
       "      <td>-0.057886</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>-0.251385</td>\n",
       "      <td>0.096959</td>\n",
       "      <td>0.079115</td>\n",
       "      <td>0.054866</td>\n",
       "      <td>-0.097657</td>\n",
       "      <td>0.223597</td>\n",
       "      <td>0.031170</td>\n",
       "      <td>0.070630</td>\n",
       "      <td>-0.136683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meekly</th>\n",
       "      <td>-0.087812</td>\n",
       "      <td>0.020389</td>\n",
       "      <td>0.040041</td>\n",
       "      <td>-0.046436</td>\n",
       "      <td>0.084847</td>\n",
       "      <td>0.022525</td>\n",
       "      <td>0.122033</td>\n",
       "      <td>-0.047317</td>\n",
       "      <td>0.005451</td>\n",
       "      <td>0.035133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059881</td>\n",
       "      <td>0.080178</td>\n",
       "      <td>0.009784</td>\n",
       "      <td>0.125705</td>\n",
       "      <td>0.010245</td>\n",
       "      <td>0.068661</td>\n",
       "      <td>-0.171023</td>\n",
       "      <td>0.034595</td>\n",
       "      <td>0.104936</td>\n",
       "      <td>0.051597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foccaccia</th>\n",
       "      <td>0.118130</td>\n",
       "      <td>0.028629</td>\n",
       "      <td>-0.063642</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>0.072471</td>\n",
       "      <td>-0.086754</td>\n",
       "      <td>0.078307</td>\n",
       "      <td>-0.052333</td>\n",
       "      <td>-0.051110</td>\n",
       "      <td>0.034550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045899</td>\n",
       "      <td>-0.123955</td>\n",
       "      <td>-0.082976</td>\n",
       "      <td>0.117231</td>\n",
       "      <td>-0.052423</td>\n",
       "      <td>-0.274294</td>\n",
       "      <td>0.093353</td>\n",
       "      <td>0.015437</td>\n",
       "      <td>-0.108372</td>\n",
       "      <td>-0.065421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clyde</th>\n",
       "      <td>0.025336</td>\n",
       "      <td>-0.044486</td>\n",
       "      <td>-0.081030</td>\n",
       "      <td>-0.049451</td>\n",
       "      <td>-0.215602</td>\n",
       "      <td>-0.004157</td>\n",
       "      <td>0.048990</td>\n",
       "      <td>-0.149425</td>\n",
       "      <td>-0.003808</td>\n",
       "      <td>-0.043461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116701</td>\n",
       "      <td>0.082913</td>\n",
       "      <td>-0.020653</td>\n",
       "      <td>-0.006104</td>\n",
       "      <td>0.045135</td>\n",
       "      <td>0.094833</td>\n",
       "      <td>-0.134000</td>\n",
       "      <td>0.077121</td>\n",
       "      <td>0.069405</td>\n",
       "      <td>0.039477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_g_spicy</th>\n",
       "      <td>0.058115</td>\n",
       "      <td>-0.036521</td>\n",
       "      <td>-0.119183</td>\n",
       "      <td>-0.040159</td>\n",
       "      <td>0.163193</td>\n",
       "      <td>0.043903</td>\n",
       "      <td>0.047436</td>\n",
       "      <td>-0.050745</td>\n",
       "      <td>-0.071357</td>\n",
       "      <td>0.027666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018813</td>\n",
       "      <td>0.062281</td>\n",
       "      <td>-0.057308</td>\n",
       "      <td>0.022598</td>\n",
       "      <td>-0.043096</td>\n",
       "      <td>0.110582</td>\n",
       "      <td>0.028887</td>\n",
       "      <td>-0.102838</td>\n",
       "      <td>0.020102</td>\n",
       "      <td>-0.127496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potaotes</th>\n",
       "      <td>0.009098</td>\n",
       "      <td>0.057424</td>\n",
       "      <td>-0.156997</td>\n",
       "      <td>-0.057388</td>\n",
       "      <td>0.030169</td>\n",
       "      <td>-0.095243</td>\n",
       "      <td>0.110111</td>\n",
       "      <td>0.049034</td>\n",
       "      <td>0.202477</td>\n",
       "      <td>0.018903</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080152</td>\n",
       "      <td>0.054386</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>-0.024257</td>\n",
       "      <td>0.078248</td>\n",
       "      <td>-0.135774</td>\n",
       "      <td>-0.086026</td>\n",
       "      <td>0.067570</td>\n",
       "      <td>0.036091</td>\n",
       "      <td>-0.236710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desert_botanical_gardens</th>\n",
       "      <td>0.073653</td>\n",
       "      <td>0.200249</td>\n",
       "      <td>-0.088580</td>\n",
       "      <td>-0.032873</td>\n",
       "      <td>-0.161853</td>\n",
       "      <td>0.066677</td>\n",
       "      <td>0.162242</td>\n",
       "      <td>-0.057449</td>\n",
       "      <td>-0.014113</td>\n",
       "      <td>0.114148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128910</td>\n",
       "      <td>-0.128824</td>\n",
       "      <td>0.168945</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>-0.043418</td>\n",
       "      <td>0.006576</td>\n",
       "      <td>-0.059663</td>\n",
       "      <td>0.228896</td>\n",
       "      <td>-0.001921</td>\n",
       "      <td>0.073541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mi_match</th>\n",
       "      <td>0.025482</td>\n",
       "      <td>0.122178</td>\n",
       "      <td>0.062693</td>\n",
       "      <td>0.150734</td>\n",
       "      <td>-0.028056</td>\n",
       "      <td>0.091268</td>\n",
       "      <td>-0.173603</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>-0.232217</td>\n",
       "      <td>-0.054804</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034624</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.182999</td>\n",
       "      <td>-0.010098</td>\n",
       "      <td>-0.074026</td>\n",
       "      <td>-0.003859</td>\n",
       "      <td>0.082882</td>\n",
       "      <td>-0.061745</td>\n",
       "      <td>0.132040</td>\n",
       "      <td>0.038518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50835 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0         1         2         3         4   \\\n",
       "the                      -0.035762 -0.173890 -0.035782 -0.007144  0.032371   \n",
       "be                       -0.074780 -0.049524  0.085974 -0.098892  0.141556   \n",
       "and                      -0.070505 -0.026918  0.028344 -0.099909  0.127974   \n",
       "i                        -0.161238  0.050831 -0.081706 -0.084479  0.053073   \n",
       "a                        -0.083491 -0.033712 -0.124125 -0.110776 -0.033046   \n",
       "to                       -0.012082  0.033135 -0.063183 -0.057252 -0.018721   \n",
       "it                        0.025022  0.081581  0.127987 -0.188015  0.041450   \n",
       "have                     -0.140812 -0.070552  0.022102  0.001077  0.109890   \n",
       "of                       -0.036341 -0.054903  0.000644 -0.010602  0.168195   \n",
       "not                      -0.075276  0.109047  0.055135  0.052251  0.209437   \n",
       "for                      -0.102976  0.001369 -0.069402 -0.122936  0.028278   \n",
       "in                       -0.053390 -0.175599 -0.091688 -0.153791  0.003205   \n",
       "we                       -0.015929 -0.019187 -0.186680 -0.240963  0.077926   \n",
       "that                     -0.026609  0.085940  0.118164  0.011576  0.156952   \n",
       "but                      -0.063436  0.089140 -0.057425 -0.093110  0.066531   \n",
       "with                      0.042318 -0.186670 -0.230563  0.076302  0.216593   \n",
       "my                       -0.002067 -0.023159  0.035879  0.036316 -0.110738   \n",
       "this                      0.139739  0.184600  0.137359 -0.109916  0.021484   \n",
       "you                      -0.171262 -0.119866  0.063801 -0.087287 -0.061923   \n",
       "on                        0.057481  0.044937 -0.063766 -0.007839  0.161119   \n",
       "they                     -0.235321 -0.026314  0.143165 -0.170460  0.042189   \n",
       "food                     -0.164133  0.007745  0.058311 -0.169839 -0.042278   \n",
       "do                        0.091428 -0.132115  0.105080  0.135949  0.038100   \n",
       "good                     -0.239592 -0.232940 -0.005036 -0.028226  0.149816   \n",
       "place                     0.025479  0.130311  0.119834 -0.096365  0.013793   \n",
       "so                        0.021455  0.079794  0.192058 -0.093809 -0.094279   \n",
       "get                       0.009313 -0.101684 -0.163864 -0.159002  0.018936   \n",
       "go                        0.031094 -0.126839 -0.054429 -0.221885 -0.063464   \n",
       "at                        0.102501 -0.095756 -0.216304 -0.107230 -0.112544   \n",
       "as                        0.141091  0.073669  0.109637 -0.112564 -0.167600   \n",
       "...                            ...       ...       ...       ...       ...   \n",
       "hard_boiled_egg           0.025154  0.060949 -0.064816  0.071975  0.087870   \n",
       "poached_quail_egg         0.060616  0.001519 -0.052908  0.013590  0.031732   \n",
       "egg_foo                   0.022816  0.077288 -0.122898 -0.022765 -0.137979   \n",
       "sherri                   -0.089536 -0.030370  0.011311 -0.094766 -0.121970   \n",
       "hindrance                -0.033325  0.011520  0.027370  0.240223 -0.004098   \n",
       "eggdrop_soup             -0.028369  0.040039 -0.005378 -0.088736  0.015203   \n",
       "arbitrarily               0.203714 -0.047405 -0.045261  0.000302 -0.074105   \n",
       "faisant                   0.046053  0.083873  0.057943  0.174203 -0.121259   \n",
       "marian                   -0.035330  0.146843 -0.173594 -0.010971 -0.150150   \n",
       "9:30p                     0.119010  0.002159 -0.000148 -0.102635 -0.016918   \n",
       "extra_chasu              -0.225889 -0.131615  0.046431  0.017999  0.119188   \n",
       "lavosh_wrap               0.134420 -0.032055  0.012240  0.024420  0.031334   \n",
       "dum_dum                  -0.002741 -0.137371  0.030704 -0.030365 -0.134645   \n",
       "triplet                  -0.010495  0.057432 -0.019535 -0.044881  0.042409   \n",
       "nantucket                -0.124356  0.141918 -0.038579  0.035650 -0.157662   \n",
       "gurl                     -0.119794  0.025898 -0.070130 -0.027929  0.113244   \n",
       "nordstroms               -0.030410 -0.026861 -0.016836  0.097363 -0.098189   \n",
       "eau_de                   -0.146450  0.000788 -0.094391  0.146833 -0.051392   \n",
       "extra_.5                 -0.124972 -0.032596 -0.017800  0.106415 -0.086728   \n",
       "hazelnut_crunch           0.055723 -0.098708  0.013225  0.098075 -0.021967   \n",
       "bella_notte               0.209296 -0.102068 -0.059274 -0.061223  0.032078   \n",
       "homebrew                  0.054390 -0.050279 -0.181006  0.001028 -0.064048   \n",
       "conveyor_belt_oven       -0.099600 -0.127618  0.070998 -0.040632 -0.022066   \n",
       "meekly                   -0.087812  0.020389  0.040041 -0.046436  0.084847   \n",
       "foccaccia                 0.118130  0.028629 -0.063642  0.006247  0.072471   \n",
       "clyde                     0.025336 -0.044486 -0.081030 -0.049451 -0.215602   \n",
       "original_g_spicy          0.058115 -0.036521 -0.119183 -0.040159  0.163193   \n",
       "potaotes                  0.009098  0.057424 -0.156997 -0.057388  0.030169   \n",
       "desert_botanical_gardens  0.073653  0.200249 -0.088580 -0.032873 -0.161853   \n",
       "mi_match                  0.025482  0.122178  0.062693  0.150734 -0.028056   \n",
       "\n",
       "                                5         6         7         8         9   \\\n",
       "the                      -0.065272 -0.219383 -0.064665  0.002739  0.025802   \n",
       "be                        0.024878 -0.011119 -0.175374  0.005410 -0.110996   \n",
       "and                      -0.058155 -0.056091 -0.028973  0.197281 -0.040528   \n",
       "i                        -0.102327 -0.108607 -0.001920 -0.057367 -0.050715   \n",
       "a                        -0.089950  0.025416 -0.052321 -0.059281  0.074985   \n",
       "to                       -0.017931 -0.027784  0.112110  0.020549 -0.174336   \n",
       "it                       -0.126222  0.172725 -0.149931 -0.069566 -0.036031   \n",
       "have                     -0.061365  0.046450  0.003073  0.113845 -0.038957   \n",
       "of                       -0.058505 -0.052342  0.039159 -0.053572 -0.160039   \n",
       "not                       0.084334 -0.122419 -0.193307  0.000699 -0.099067   \n",
       "for                      -0.074256 -0.013786 -0.147065  0.204125 -0.033473   \n",
       "in                        0.013146 -0.013261  0.162506 -0.036985 -0.123813   \n",
       "we                       -0.122313 -0.183584 -0.038707  0.067121 -0.108626   \n",
       "that                     -0.061402 -0.068207  0.008184 -0.169472  0.051105   \n",
       "but                      -0.079715 -0.049745 -0.161346  0.097094  0.035439   \n",
       "with                     -0.056183  0.004471 -0.087819  0.073513 -0.219137   \n",
       "my                       -0.033034 -0.100291 -0.039403  0.109342  0.024952   \n",
       "this                     -0.018423 -0.027546 -0.055886 -0.137625 -0.058589   \n",
       "you                       0.023105 -0.196524 -0.043654 -0.003327 -0.078496   \n",
       "on                       -0.047322 -0.024250 -0.038904  0.085989  0.036280   \n",
       "they                     -0.019444 -0.171945 -0.087666  0.005467 -0.034397   \n",
       "food                      0.004095  0.203732 -0.021252 -0.084491 -0.016372   \n",
       "do                        0.066993 -0.046825 -0.165575 -0.087334  0.068053   \n",
       "good                     -0.133312 -0.034164 -0.130310 -0.013757  0.008618   \n",
       "place                     0.074431 -0.063780  0.063191 -0.004273  0.111458   \n",
       "so                       -0.147522 -0.066564 -0.073133  0.009708  0.050529   \n",
       "get                      -0.056202 -0.074619 -0.127081  0.182303 -0.001993   \n",
       "go                        0.024554  0.060154 -0.011108 -0.020744  0.038979   \n",
       "at                       -0.036979 -0.066605 -0.016080  0.046475 -0.128300   \n",
       "as                       -0.059139 -0.122552 -0.137383  0.093218  0.096284   \n",
       "...                            ...       ...       ...       ...       ...   \n",
       "hard_boiled_egg          -0.034552  0.046470 -0.074013  0.048614 -0.027098   \n",
       "poached_quail_egg        -0.020164  0.067209  0.015895  0.034758 -0.247553   \n",
       "egg_foo                  -0.131516 -0.025750  0.071284 -0.130913 -0.041167   \n",
       "sherri                    0.143681 -0.035314 -0.060650  0.005531 -0.034962   \n",
       "hindrance                -0.047842 -0.008730 -0.086408 -0.155469  0.036887   \n",
       "eggdrop_soup             -0.150734 -0.008286  0.052836  0.041498  0.061759   \n",
       "arbitrarily              -0.011420  0.006737  0.068212 -0.010306  0.162682   \n",
       "faisant                  -0.043806 -0.069513 -0.037047 -0.026478 -0.119066   \n",
       "marian                    0.082224  0.036275 -0.028033 -0.076082  0.051976   \n",
       "9:30p                    -0.075822 -0.016008  0.012505 -0.068003 -0.066301   \n",
       "extra_chasu              -0.075226 -0.140749 -0.054829  0.210201 -0.098395   \n",
       "lavosh_wrap              -0.002414  0.022550 -0.024545  0.054083  0.101219   \n",
       "dum_dum                  -0.036521 -0.019889 -0.191169  0.034061  0.156001   \n",
       "triplet                  -0.094355  0.111214 -0.141414 -0.102281  0.013674   \n",
       "nantucket                 0.048110 -0.006915  0.049056  0.191926  0.001897   \n",
       "gurl                      0.076868  0.084859 -0.000508 -0.008275  0.026478   \n",
       "nordstroms                0.080675  0.005855  0.065331 -0.047586 -0.083942   \n",
       "eau_de                    0.026519  0.022764 -0.161101 -0.106951  0.018758   \n",
       "extra_.5                 -0.039636 -0.048088 -0.016923 -0.079315  0.078559   \n",
       "hazelnut_crunch          -0.046137  0.083640  0.011891 -0.034513  0.159220   \n",
       "bella_notte               0.042433  0.060573 -0.263855  0.026291 -0.082852   \n",
       "homebrew                  0.029383  0.007203 -0.207681 -0.026785  0.004940   \n",
       "conveyor_belt_oven       -0.021832 -0.066087 -0.130704 -0.057886  0.000509   \n",
       "meekly                    0.022525  0.122033 -0.047317  0.005451  0.035133   \n",
       "foccaccia                -0.086754  0.078307 -0.052333 -0.051110  0.034550   \n",
       "clyde                    -0.004157  0.048990 -0.149425 -0.003808 -0.043461   \n",
       "original_g_spicy          0.043903  0.047436 -0.050745 -0.071357  0.027666   \n",
       "potaotes                 -0.095243  0.110111  0.049034  0.202477  0.018903   \n",
       "desert_botanical_gardens  0.066677  0.162242 -0.057449 -0.014113  0.114148   \n",
       "mi_match                  0.091268 -0.173603  0.027926 -0.232217 -0.054804   \n",
       "\n",
       "                            ...           90        91        92        93  \\\n",
       "the                         ...     0.050136  0.044030  0.145281 -0.020442   \n",
       "be                          ...    -0.199047 -0.081284 -0.198344  0.007257   \n",
       "and                         ...    -0.049051 -0.212434 -0.042576  0.055731   \n",
       "i                           ...     0.028528 -0.016578 -0.179229  0.053357   \n",
       "a                           ...    -0.101939  0.022392  0.057049  0.015819   \n",
       "to                          ...    -0.017111 -0.067532 -0.022149  0.154788   \n",
       "it                          ...     0.045720  0.094828  0.089329  0.051623   \n",
       "have                        ...    -0.051071 -0.090922 -0.022011  0.157082   \n",
       "of                          ...     0.085908 -0.211464 -0.084990  0.082315   \n",
       "not                         ...    -0.150619 -0.060446  0.181940 -0.118538   \n",
       "for                         ...     0.032123  0.013365  0.008156 -0.021331   \n",
       "in                          ...    -0.101997 -0.025117  0.101147  0.002555   \n",
       "we                          ...     0.054799 -0.029601 -0.197221 -0.081994   \n",
       "that                        ...    -0.088909  0.062827 -0.114507  0.007300   \n",
       "but                         ...    -0.129163 -0.022460 -0.200731  0.079950   \n",
       "with                        ...     0.036514  0.135176 -0.056771 -0.020261   \n",
       "my                          ...    -0.051417  0.220378 -0.106171  0.159718   \n",
       "this                        ...    -0.035842 -0.075413 -0.068598  0.122231   \n",
       "you                         ...     0.094615  0.029243  0.020553 -0.101657   \n",
       "on                          ...     0.084352 -0.119525  0.076835 -0.010369   \n",
       "they                        ...    -0.122975 -0.054745  0.022250 -0.068428   \n",
       "food                        ...    -0.096869  0.060159 -0.133541  0.166804   \n",
       "do                          ...    -0.101949 -0.037880 -0.187836  0.037602   \n",
       "good                        ...    -0.034984 -0.135347 -0.112965  0.056312   \n",
       "place                       ...    -0.076251 -0.076574 -0.086146 -0.023936   \n",
       "so                          ...    -0.050627 -0.008651 -0.034267  0.045445   \n",
       "get                         ...    -0.040294 -0.038149 -0.180993 -0.143341   \n",
       "go                          ...    -0.074458 -0.172092 -0.123518  0.006400   \n",
       "at                          ...    -0.058233 -0.046821  0.042406  0.178607   \n",
       "as                          ...    -0.052156 -0.106116 -0.088926 -0.079129   \n",
       "...                         ...          ...       ...       ...       ...   \n",
       "hard_boiled_egg             ...     0.161784 -0.011800  0.036288  0.102444   \n",
       "poached_quail_egg           ...     0.074396  0.051672  0.038299 -0.001248   \n",
       "egg_foo                     ...     0.044335  0.100112  0.124033 -0.012426   \n",
       "sherri                      ...    -0.010114  0.059706  0.068094  0.158161   \n",
       "hindrance                   ...     0.028251  0.136807  0.079745 -0.107322   \n",
       "eggdrop_soup                ...     0.058192  0.170658 -0.069061 -0.043249   \n",
       "arbitrarily                 ...    -0.165282 -0.078750 -0.051527  0.129190   \n",
       "faisant                     ...    -0.063223  0.018042 -0.107165  0.028304   \n",
       "marian                      ...     0.042506  0.160951  0.003764  0.168254   \n",
       "9:30p                       ...     0.027208 -0.110685  0.139527 -0.031720   \n",
       "extra_chasu                 ...     0.049070 -0.028876 -0.173717  0.074353   \n",
       "lavosh_wrap                 ...     0.092470 -0.109932  0.019652 -0.090741   \n",
       "dum_dum                     ...     0.072851  0.016341 -0.009848  0.038048   \n",
       "triplet                     ...     0.009309 -0.071423  0.029983 -0.079734   \n",
       "nantucket                   ...    -0.076790 -0.067047  0.020261  0.088759   \n",
       "gurl                        ...     0.038612  0.010746  0.052930  0.236658   \n",
       "nordstroms                  ...    -0.029618 -0.259797  0.078861  0.081747   \n",
       "eau_de                      ...    -0.051030 -0.074207 -0.017950 -0.091572   \n",
       "extra_.5                    ...    -0.180088  0.053834 -0.146145  0.114461   \n",
       "hazelnut_crunch             ...     0.098174  0.030037 -0.028975  0.043462   \n",
       "bella_notte                 ...    -0.089840 -0.048790 -0.040089 -0.112727   \n",
       "homebrew                    ...    -0.138264 -0.097649  0.139629  0.101149   \n",
       "conveyor_belt_oven          ...     0.020469 -0.251385  0.096959  0.079115   \n",
       "meekly                      ...     0.059881  0.080178  0.009784  0.125705   \n",
       "foccaccia                   ...     0.045899 -0.123955 -0.082976  0.117231   \n",
       "clyde                       ...    -0.116701  0.082913 -0.020653 -0.006104   \n",
       "original_g_spicy            ...     0.018813  0.062281 -0.057308  0.022598   \n",
       "potaotes                    ...    -0.080152  0.054386  0.005011 -0.024257   \n",
       "desert_botanical_gardens    ...    -0.128910 -0.128824  0.168945  0.011161   \n",
       "mi_match                    ...    -0.034624  0.001389  0.182999 -0.010098   \n",
       "\n",
       "                                94        95        96        97        98  \\\n",
       "the                       0.128879 -0.076461  0.075532 -0.012841  0.024710   \n",
       "be                        0.075339  0.070266 -0.008326 -0.127542 -0.046246   \n",
       "and                       0.117097 -0.206737  0.055435 -0.065056  0.052316   \n",
       "i                         0.070913  0.036893 -0.000544 -0.007254 -0.056005   \n",
       "a                        -0.001798  0.001103  0.003096  0.037175 -0.074279   \n",
       "to                       -0.093789 -0.020456  0.065478  0.075484 -0.053530   \n",
       "it                       -0.108989 -0.145476  0.068617  0.090687 -0.101725   \n",
       "have                     -0.082406 -0.010306 -0.063481 -0.098728 -0.064020   \n",
       "of                        0.223018 -0.142501  0.280647  0.003435 -0.037710   \n",
       "not                      -0.002879  0.018827  0.084586  0.040437  0.070277   \n",
       "for                       0.025385  0.105075  0.184737  0.087325 -0.230621   \n",
       "in                       -0.075434 -0.031021  0.170358 -0.070997 -0.143472   \n",
       "we                        0.114129  0.127746  0.057743 -0.044793 -0.080014   \n",
       "that                     -0.075059 -0.202200  0.003658  0.042448 -0.091925   \n",
       "but                      -0.002590 -0.113734  0.048470  0.037333  0.111525   \n",
       "with                      0.213735 -0.116074  0.162992  0.015298 -0.152731   \n",
       "my                       -0.036391 -0.025573  0.133651 -0.157615  0.010161   \n",
       "this                     -0.097841  0.114074  0.111075  0.174843 -0.018743   \n",
       "you                       0.039655  0.059782 -0.073931 -0.002060 -0.068405   \n",
       "on                        0.035561  0.055588  0.119598  0.306402 -0.095085   \n",
       "they                     -0.009932 -0.012489  0.102740  0.071282 -0.165166   \n",
       "food                      0.084901  0.109261  0.137871  0.018093 -0.158754   \n",
       "do                       -0.094156 -0.040069 -0.013014 -0.013038 -0.033346   \n",
       "good                      0.055106 -0.026181 -0.135510  0.087664  0.009934   \n",
       "place                     0.136419 -0.001543 -0.084301  0.016356 -0.148379   \n",
       "so                       -0.104442 -0.012076 -0.118052 -0.015163 -0.006679   \n",
       "get                       0.140279  0.181399  0.054530 -0.152596  0.028443   \n",
       "go                       -0.085149  0.157569 -0.048633  0.017931  0.111066   \n",
       "at                        0.181424 -0.120113  0.029031  0.113648 -0.107441   \n",
       "as                        0.072921 -0.009605 -0.001447  0.068642 -0.022845   \n",
       "...                            ...       ...       ...       ...       ...   \n",
       "hard_boiled_egg          -0.036660 -0.259496  0.093633 -0.055128  0.099477   \n",
       "poached_quail_egg         0.074726 -0.081252 -0.078778  0.008109  0.023891   \n",
       "egg_foo                   0.037161  0.058809 -0.033203 -0.105034  0.207285   \n",
       "sherri                   -0.076122  0.115804 -0.133826  0.022022 -0.115043   \n",
       "hindrance                -0.092331  0.151693  0.126414 -0.036642  0.043212   \n",
       "eggdrop_soup              0.080981 -0.074859  0.026031 -0.048331  0.196082   \n",
       "arbitrarily              -0.088332  0.000339  0.021954  0.224878 -0.020637   \n",
       "faisant                  -0.141706 -0.084532  0.097593 -0.029115 -0.016920   \n",
       "marian                   -0.057206 -0.067292 -0.254453  0.049995 -0.097059   \n",
       "9:30p                    -0.101919  0.105827  0.034134  0.083859  0.089299   \n",
       "extra_chasu              -0.078363 -0.166292 -0.007546  0.116509  0.073052   \n",
       "lavosh_wrap              -0.008825  0.052382  0.012688 -0.035351 -0.093695   \n",
       "dum_dum                  -0.026917 -0.035949 -0.022561 -0.000162  0.026049   \n",
       "triplet                   0.017278  0.049596  0.000595 -0.111090  0.125764   \n",
       "nantucket                 0.029744  0.020393 -0.033682  0.150856  0.276557   \n",
       "gurl                      0.021199 -0.092340 -0.143270  0.038394  0.005431   \n",
       "nordstroms               -0.147309  0.084849 -0.121320  0.219587 -0.045757   \n",
       "eau_de                   -0.142917 -0.220402  0.008315 -0.086124  0.101774   \n",
       "extra_.5                  0.028400  0.033901  0.040767  0.131177  0.154335   \n",
       "hazelnut_crunch           0.046888 -0.187656  0.048226 -0.086743  0.060892   \n",
       "bella_notte              -0.008889 -0.095856 -0.047674  0.194045 -0.118062   \n",
       "homebrew                 -0.145391  0.143334  0.121490 -0.018492  0.134825   \n",
       "conveyor_belt_oven        0.054866 -0.097657  0.223597  0.031170  0.070630   \n",
       "meekly                    0.010245  0.068661 -0.171023  0.034595  0.104936   \n",
       "foccaccia                -0.052423 -0.274294  0.093353  0.015437 -0.108372   \n",
       "clyde                     0.045135  0.094833 -0.134000  0.077121  0.069405   \n",
       "original_g_spicy         -0.043096  0.110582  0.028887 -0.102838  0.020102   \n",
       "potaotes                  0.078248 -0.135774 -0.086026  0.067570  0.036091   \n",
       "desert_botanical_gardens -0.043418  0.006576 -0.059663  0.228896 -0.001921   \n",
       "mi_match                 -0.074026 -0.003859  0.082882 -0.061745  0.132040   \n",
       "\n",
       "                                99  \n",
       "the                      -0.067555  \n",
       "be                        0.110279  \n",
       "and                      -0.078666  \n",
       "i                         0.106345  \n",
       "a                         0.001683  \n",
       "to                       -0.005314  \n",
       "it                        0.090377  \n",
       "have                      0.153466  \n",
       "of                       -0.145140  \n",
       "not                      -0.047521  \n",
       "for                       0.075051  \n",
       "in                       -0.039543  \n",
       "we                       -0.001816  \n",
       "that                      0.045213  \n",
       "but                      -0.001558  \n",
       "with                      0.070306  \n",
       "my                       -0.172925  \n",
       "this                      0.087721  \n",
       "you                      -0.246893  \n",
       "on                        0.053575  \n",
       "they                      0.126805  \n",
       "food                     -0.042917  \n",
       "do                       -0.056112  \n",
       "good                     -0.111619  \n",
       "place                    -0.016498  \n",
       "so                       -0.074553  \n",
       "get                      -0.030319  \n",
       "go                        0.040107  \n",
       "at                       -0.005374  \n",
       "as                        0.197407  \n",
       "...                            ...  \n",
       "hard_boiled_egg           0.069598  \n",
       "poached_quail_egg         0.075863  \n",
       "egg_foo                  -0.094321  \n",
       "sherri                    0.043500  \n",
       "hindrance                -0.016060  \n",
       "eggdrop_soup              0.050968  \n",
       "arbitrarily               0.019025  \n",
       "faisant                  -0.027668  \n",
       "marian                    0.106862  \n",
       "9:30p                     0.053762  \n",
       "extra_chasu               0.090262  \n",
       "lavosh_wrap              -0.105152  \n",
       "dum_dum                   0.074344  \n",
       "triplet                  -0.020409  \n",
       "nantucket                -0.086213  \n",
       "gurl                      0.177151  \n",
       "nordstroms               -0.032065  \n",
       "eau_de                    0.051059  \n",
       "extra_.5                  0.170376  \n",
       "hazelnut_crunch           0.115703  \n",
       "bella_notte               0.139320  \n",
       "homebrew                  0.120686  \n",
       "conveyor_belt_oven       -0.136683  \n",
       "meekly                    0.051597  \n",
       "foccaccia                -0.065421  \n",
       "clyde                     0.039477  \n",
       "original_g_spicy         -0.127496  \n",
       "potaotes                 -0.236710  \n",
       "desert_botanical_gardens  0.073541  \n",
       "mi_match                  0.038518  \n",
       "\n",
       "[50835 rows x 100 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a list of the terms, integer indices,\n",
    "# and term counts from the food2vec model vocabulary\n",
    "ordered_vocab = [(term, voc.index, voc.count)\n",
    "                 for term, voc in food2vec.vocab.iteritems()]\n",
    "\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda (term, index, count): -count)\n",
    "\n",
    "# unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# create a DataFrame with the food2vec vectors as data,\n",
    "# and the terms as row labels\n",
    "word_vectors = pd.DataFrame(food2vec.syn0norm[term_indices, :],\n",
    "                            index=ordered_terms)\n",
    "\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy wall of numbers! This DataFrame has 50,835 rows &mdash; one for each term in the vocabulary &mdash; and 100 colums. Our model has learned a quantitative vector representation for each term, as expected.\n",
    "\n",
    "Put another way, our model has \"embedded\" the terms into a 100-dimensional vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So... what can we do with all these numbers?\n",
    "The first thing we can use them for is to simply look up related words and phrases for a given term of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_related_terms(token, topn=10):\n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token\n",
    "    and print them as a formatted list\n",
    "    \"\"\"\n",
    "\n",
    "    for word, similarity in food2vec.most_similar(positive=[token], topn=topn):\n",
    "\n",
    "        print u'{:20} {}'.format(word, round(similarity, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
