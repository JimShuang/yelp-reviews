{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Yelp Dataset\n",
    "[**The Yelp Dataset**](https://www.yelp.com/dataset_challenge/) is a dataset published by the business review service [Yelp](http://yelp.com) for academic research and educational purposes. I really like the Yelp dataset as a subject for machine learning and natural language processing demos, because it's big (but not so big that you need your own data center to process it), well-connected, and anyone can relate to it &mdash; it's largely about food, after all!\n",
    "\n",
    "**Note:** If you'd like to execute this notebook interactively on your local machine, you'll need to download your own copy of the Yelp dataset. If you're reviewing a static copy of the notebook online, you can skip this step. Here's how to get the dataset:\n",
    "1. Please visit the Yelp dataset webpage [here](https://www.yelp.com/dataset_challenge/)\n",
    "1. Click \"Get the Data\"\n",
    "1. Please review, agree to, and respect Yelp's terms of use!\n",
    "1. The dataset downloads as a compressed .tgz file; uncompress it\n",
    "1. Place the uncompressed dataset files (*yelp_academic_dataset_business.json*, etc.) in a directory named *yelp_dataset_challenge_academic_dataset*\n",
    "1. Place the *yelp_dataset_challenge_academic_dataset* within the *data* directory in the *Modern NLP in Python* project folder\n",
    "\n",
    "That's it! You're ready to go.\n",
    "\n",
    "The current iteration of the Yelp dataset (as of this demo) consists of the following data:\n",
    "- __552K__ users\n",
    "- __77K__ businesses\n",
    "- __2.2M__ user reviews\n",
    "\n",
    "When focusing on restaurants alone, there are approximately __22K__ restaurants with approximately __1M__ user reviews written about them.\n",
    "\n",
    "The data is provided in a handful of files in _.json_ format. We'll be using the following files for our demo:\n",
    "- __yelp\\_academic\\_dataset\\_business.json__ &mdash; _the records for individual businesses_\n",
    "- __yelp\\_academic\\_dataset\\_review.json__ &mdash; _the records for reviews users wrote about businesses_\n",
    "\n",
    "The files are text files (UTF-8) with one _json object_ per line, each one corresponding to an individual data record. Let's take a look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import codecs\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 42\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review records are stored in a similar manner &mdash; _key, value_ pairs containing information about the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"VfBHSwC5Vz_pbFluy07i9Q\",\"user_id\":\"cjpdDjZyprfyDG3RlkVG3w\",\"business_id\":\"uYHaNptLzDLoV_JZ_MuzUA\",\"stars\":5,\"date\":\"2016-07-12\",\"text\":\"My girlfriend and I stayed here for 3 nights and loved it. The location of this hotel and very decent price makes this an amazing deal. When you walk out the front door Scott Monument and Princes street are right in front of you, Edinburgh Castle and the Royal Mile is a 2 minute walk via a close right around the corner, and there are so many hidden gems nearby including Calton Hill and the newly opened Arches that made this location incredible.\\n\\nThe hotel itself was also very nice with a reasonably priced bar, very considerate staff, and small but comfortable rooms with excellent bathrooms and showers. Only two minor complaints are no telephones in room for room service (not a huge deal for us) and no AC in the room, but they have huge windows which can be fully opened. The staff were incredible though, letting us borrow umbrellas for the rain, giving us maps and directions, and also when we had lost our only UK adapter for charging our phones gave us a very fancy one for free.\\n\\nI would highly recommend this hotel to friends, and when I return to Edinburgh (which I most definitely will) I will be staying here without any hesitation.\",\"useful\":0,\"funny\":0,\"cool\":0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_dir = os.path.join('..', 'data',\n",
    "                              'yelp_dataset_challenge_academic_dataset', 'dataset')\n",
    "\n",
    "json_review_filepath = os.path.join(json_dir,\n",
    "                                    'review.json')\n",
    "\n",
    "with open(json_review_filepath, encoding='utf_8') as f:\n",
    "    first_review_record = f.readline()\n",
    "    \n",
    "print(first_review_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few attributes of note on the review records:\n",
    "- __text__ &mdash; _the natural language text the user wrote_\n",
    "- __stars__ &mdash; _the number of stars the reviewer left_\n",
    "\n",
    "The _text_ and the _stars_ attribute will be our focus today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a new directory that contains only the text from reviews about restaurants, with one review per line in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_data_dir = os.path.join('..','data','sentiment_data')\n",
    "\n",
    "text_filepath = os.path.join(sentiment_data_dir,'sentiment.txt')\n",
    "sentiment_filepath = os.path.join(sentiment_data_dir,'number_of_stars.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 4736897 reviews written to the new txt file.\n",
      "CPU times: user 1min 6s, sys: 17.4 s, total: 1min 24s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make the if statement True\n",
    "# if you want to execute data prep yourself once you've got the yelp dataset saved.\n",
    "\n",
    "if True:\n",
    "    \n",
    "    review_count = 0\n",
    "\n",
    "    # create & open a new files in write mode\n",
    "    with open(text_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "        with open(sentiment_filepath, 'w', encoding='utf_8') as review_sentiment_file:\n",
    "\n",
    "            # open the existing review json file\n",
    "            with open(json_review_filepath, encoding='utf_8') as review_json_file:\n",
    "                # loop through all reviews in the existing file and convert to dict\n",
    "                for review_json in review_json_file:\n",
    "                    review = json.loads(review_json)\n",
    "                    # write the review as a line in the new file\n",
    "                    # escape newline characters in the original review text\n",
    "                    review_txt_file.write(review.get('text','NA').replace('\\n', r'\\n') + '\\n')\n",
    "                    review_sentiment_file.write(str(review.get('stars','NA')) +'\\n')\n",
    "                    review_count =  review_count + 1\n",
    "\n",
    "    print ('Text from {} reviews written to the new txt file.'.format(review_count))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with open(text_filepath, encoding='utf_8') as review_txt_file:\n",
    "        for review_count, line in enumerate(review_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print('Text from {} reviews in the txt file.'.format(review_count + 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #count the lines in the above files\n",
    "\n",
    "# from itertools import (takewhile,repeat)\n",
    "\n",
    "# def rawincount(filename):\n",
    "#     with open(filename, 'rb') as f:\n",
    "#         bufgen = takewhile(lambda x: x, (f.raw.read(1024*1024) for _ in repeat(None)))\n",
    "#         return sum( buf.count(b'\\n') for buf in bufgen )\n",
    "\n",
    "# print('Len of review text file:{}\\nLen of review sentiment file:{}'.format(rawincount(review_txt_filepath), rawincount(review_sentiment_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Good!  The lengths of the files match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets do train-test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_train_dir = os.path.join(sentiment_data_dir, 'train')\n",
    "sentiment_test_dir = os.path.join(sentiment_data_dir, 'test')\n",
    "\n",
    "X_train_file_path = os.path.join(sentiment_train_dir, 'X_train.txt')\n",
    "y_train_file_path = os.path.join(sentiment_train_dir, 'y_train.txt')\n",
    "X_test_file_path = os.path.join(sentiment_test_dir, 'X_test.txt')\n",
    "y_test_file_path = os.path.join(sentiment_test_dir, 'y_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/sentiment_data/number_of_stars.txt'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_stars.txt  \u001b[34mtest\u001b[m\u001b[m/\r\n",
      "sentiment.txt        \u001b[34mtrain\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls ../data/sentiment_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/sentiment_data/sentiment.txt'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 seanreed1  staff   2.8G Nov  7 11:48 ../data/sentiment_data/sentiment.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ../data/sentiment_data/sentiment.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build a mini dictionary\n",
    "with open(sentiment_filepath, 'r') as f:\n",
    "    y = [line.rstrip() for num,line in enumerate(f) if num < 1000 ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(text_filepath, 'r') as f:\n",
    "    X = [line.rstrip() for num,line in enumerate(f) if num < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My girlfriend and I stayed here for 3 nights and loved it. The location of this hotel and very decent price makes this an amazing deal. When you walk out the front door Scott Monument and Princes street are right in front of you, Edinburgh Castle and the Royal Mile is a 2 minute walk via a close right around the corner, and there are so many hidden gems nearby including Calton Hill and the newly opened Arches that made this location incredible.\\\\n\\\\nThe hotel itself was also very nice with a reasonably priced bar, very considerate staff, and small but comfortable rooms with excellent bathrooms and showers. Only two minor complaints are no telephones in room for room service (not a huge deal for us) and no AC in the room, but they have huge windows which can be fully opened. The staff were incredible though, letting us borrow umbrellas for the rain, giving us maps and directions, and also when we had lost our only UK adapter for charging our phones gave us a very fancy one for free.\\\\n\\\\nI would highly recommend this hotel to friends, and when I return to Edinburgh (which I most definitely will) I will be staying here without any hesitation.',\n",
       " \"If you need an inexpensive place to stay for a night or two then you may consider this place but for a longer stay I'd recommend somewhere with better amenities. \\\\n\\\\nPros:\\\\nGreat location- you're right by the train station, central location to get to old town and new town, and right by sight seeing his tours. Food, bars, and shopping all within walking distance. Location, location, location.\\\\nVery clean and very good maid service\\\\n\\\\nCons:\\\\nTiny rooms \\\\nUncomfortable bed \\\\nAbsolutely no amenities \\\\nNo phone in room \\\\nNo wardrobe \\\\n\\\\nWas given a lot of attitude about me and my husband sharing a room which was quite strange and we were charged 15 pounds more for double occupancy not sure why that matters I felt like it was a money grab. It was just handled in a kind of odd manner to me... \\\\n\\\\nIf you book this hotel all you get is a bed, desk, and a bathroom. It isn't awful but know what you're getting into.\",\n",
       " \"Mittlerweile gibt es in Edinburgh zwei Ableger der Motel-One-Kette - diese hier ist eher das schlechtere.\\\\n\\\\nWir hatten auf unserer Schottlandtour die Gelegenheit beide Ableger kennenzulernen, da wir beide Nächte in Edinburgh (am ersten sowie am letzten Tag) in einem der beiden Motel One's verbrachten. Von diesem hier waren wir nach dem überraschend positiven Erlebnis im Motel One an der Princes Street [1] etwas enttäuscht.\\\\n\\\\nDas Hotel ist wesentlich größer und extrem unübersichtlich. Da man auf der fünften Etage noch Teile eines weiteren Gebäude, das mittels einer Glasbrücke mit dem Hauptgebäude verbunden ist, hinzugenommen hat, kann die Suche nach dem Zimmer insbesondere auf dieser Etage zu einem Gang im Labyrinth ausarten. Habe ich so noch in keinem Hotel erlebt - uns begegneten immer wieder Menschen, die sich auf der Suche nach ihren Zimmern verirrt haben.\\\\nUnser Zimmer war ähnlich aufgebaut wie das Zimmer im Motel One Princes Street - befand sich jedoch im Inneren des Gebäudes und hatte daher kein Fenster. Nur eine kleine Dachluke, die sich am oberen Ende eines etwa 3m langen Schachts befand, sorgte für einen minimalen Anteil Tageslicht im Zimmer. diese Luke ließ sich auch nicht öffnen, nur ein elektrisches Rollo ließ sich mittels eines Schalters öffnen bzw. schließen.\\\\nSonst gilt für das Zimmer eigentlich das Gleiche wie für das Zimmer im im Motel One Princes Street: Relativ klein, trotzdem extrem ansprechend und schick gestaltet und clever durchdacht. Dazu sehr ruhig.\\\\n\\\\nIm gesamten Hotel gibt es kostenloses WLAN für die Dauer des Aufenthalts für maximal 3 Geräte.\\\\n\\\\nDie Lage des Motel One Royal Mile ist großartig. Vom Airlink-Flughafenbus kann man quasi in den Hoteleingang stolpern, es sind maximal 50m. Zur Royal Mile sind es etwa 100m Luftlinie (bergauf) und man benötigt etwa 5 Minuten bis dort.\\\\n\\\\nDas Frühstück für 9,50 GBP / Person kann man nutzen - muss man aber nicht. Es ist eher übersichtlich und kontinetaler Prägung, trotzdem aber sehr gut. Für den gleichen Preis oder weniger bekommt man aber problemlos ein reichhaltiges Frühstück in einem der zahlreichen Cafes von Edinburgh.\\\\n\\\\nFür ein oder zwei Nächte kann man hier problemlos bleiben, sollte aber nicht allzuviel erwarten. Hat man die Wahl, sollte man meiner Erfahrung nach lieber auf das andere Motel One an der Princes Street ausweichen. \\\\n\\\\n\\\\n----\\\\n[1] http://www.yelp.de/biz/motel-one-edinburgh-2?hrid=7UDwl3s_DuOokkGPZXZgBw\",\n",
       " \"Location is everything and this hotel has it! The reception is inviting and open 24 hours. They are very helpful and have a lot of patience answering all my questions about where to go etc. there is also a lounge open 24 hours with snack-type food. Breakfast is continental-style so if you want heartier fare look elsewhere though you don't have to go far. The bus and train stations are right across the street so it's easy access to the airport or anywhere else you may want to go. Turn uphill to old town or cross the bridge to new town. The room with a view i got was spacious and comfortable though it's a bit of a maze to find it-just follow the signs. The windows are double paned so the room is quiet plus i was on the 5th floor which helps. It's a bit pricey but still one of the best values i found!\",\n",
       " 'gute lage im stadtzentrum. shoppingmeile und sehenswürdigkeiten, sowie gute pubs in laufweite. das hotel ist neu, gut gepflegt und hat bemühtes/nettes personal. ideal für einen kurztrip nach edinburgh. längere aufenthalte eher nicht, da die zimmer recht klein sind.',\n",
       " 'Erstklassige Lage. Dazu ist alles geschrieben  worden.\\\\nWir hatten Zimmer 718 mit Blick auf den Bahnhof. Ist zwar zu laut mit offenen Fenster nachts, aber der Ausblick verzeiht alles. Das Personal ist sehr freundlich und super hilfsbereit. Wer Edinburgh zu Fuß erkunden möchte, ist hier perfekt aufgehoben.',\n",
       " 'Beautiful space, great location, staff rock. Tiny room, but this was expected. Bathroom amazing. Walls, however, paper thin, which is why I can barely string a sentence together in this review.',\n",
       " \"This is a fairly new property I think. It is a German company and has most of the amenities you would want. It is priced on the budget minded side so it won't break your bank.\\\\nLocation is really good. Near the Royal Mile and Waverley station without being too noisy. Very easy to walk to everything we wanted to do. Has WiFi but we did have to re-log in every day.\",\n",
       " 'First time at this group of hotels. Pretty new, only one in UK, another to open in Edinburgh and one in London. Rooms not very big but great price and location for a weekend in Edinburgh. Rooms clean, comfortable, good shower and free wifi!',\n",
       " \"Location location location! \\\\n\\\\nMotel One is just right in the middle of everything. To one side you have the New town and the main trains station ( and right in front of the airport bus)  and on the other side the beautiful old town of Edinburg. \\\\n\\\\nThe room was small but clean and with all you need. \\\\nThe bathroom was also small but nice and clean and the shower was big and good! Next to the bed there is a European plug (CC 7/7) so we actually didn't need to use our adapter. \\\\n\\\\nThe beed was to hard for my back and I prefer the bed more soft, but I will still come back to Edinburgh and Motel One.\"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=SEED)\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy &mdash; Industrial-Strength NLP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spaCy](https://s3.amazonaws.com/skipgram-images/spaCy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**spaCy**](https://spacy.io) is an industrial-strength natural language processing (_NLP_) library for Python. spaCy's goal is to take recent advancements in natural language processing out of research papers and put them in the hands of users to build production software.\n",
    "\n",
    "spaCy handles many tasks commonly associated with building an end-to-end natural language processing pipeline:\n",
    "- Tokenization\n",
    "- Text normalization, such as lowercasing, stemming/lemmatization\n",
    "- Part-of-speech tagging\n",
    "- Syntactic dependency parsing\n",
    "- Sentence boundary detection\n",
    "- Named entity recognition and annotation\n",
    "\n",
    "In the \"batteries included\" Python tradition, spaCy contains built-in data and models which you can use out-of-the-box for processing general-purpose English language text:\n",
    "- Large English vocabulary, including stopword lists\n",
    "- Token \"probabilities\"\n",
    "- Word vectors\n",
    "\n",
    "spaCy is written in optimized Cython, which means it's _fast_. According to a few independent sources, it's the fastest syntactic parser available in any language. Key pieces of the spaCy parsing pipeline are written in pure C, enabling efficient multithreading (i.e., spaCy can release the _GIL_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_md\n",
    "#!python -m spacy link en_core_web_md en_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "nlp = spacy.load('en_default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab a sample review to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(review_txt_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 8, 9))[0]\n",
    "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
    "        \n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector Embedding with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop quiz! Can you complete this text snippet?\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word2vec quiz](https://s3.amazonaws.com/skipgram-images/word2vec-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "You just demonstrated the core machine learning concept behind word vector embedding models!\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word2vec quiz 2](https://s3.amazonaws.com/skipgram-images/word2vec-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of *word vector embedding models*, or *word vector models* for short, is to learn dense, numerical vector representations for each term in a corpus vocabulary. If the model is successful, the vectors it learns about each term should encode some information about the *meaning* or *concept* the term represents, and the relationship between it and other terms in the vocabulary. Word vector models are also fully unsupervised &mdash; they learn all of these meanings and relationships solely by analyzing the text of the corpus, without any advance knowledge provided.\n",
    "\n",
    "Perhaps the best-known word vector model is [word2vec](https://arxiv.org/pdf/1301.3781v3.pdf), originally proposed in 2013. The general idea of word2vec is, for a given *focus word*, to use the *context* of the word &mdash; i.e., the other words immediately before and after it &mdash; to provide hints about what the focus word might mean. To do this, word2vec uses a *sliding window* technique, where it considers snippets of text only a few tokens long at a time.\n",
    "\n",
    "At the start of the learning process, the model initializes random vectors for all terms in the corpus vocabulary. The model then slides the window across every snippet of text in the corpus, with each word taking turns as the focus word. Each time the model considers a new snippet, it tries to learn some information about the focus word based on the surrouding context, and it \"nudges\" the words' vector representations accordingly. One complete pass sliding the window across all of the corpus text is known as a training *epoch*. It's common to train a word2vec model for multiple passes/epochs over the corpus. Over time, the model rearranges the terms' vector representations such that terms that frequently appear in similar contexts have vector representations that are *close* to each other in vector space.\n",
    "\n",
    "For a deeper dive into word2vec's machine learning process, see [here](https://arxiv.org/pdf/1411.2738v4.pdf).\n",
    "\n",
    "Word2vec has a number of user-defined hyperparameters, including:\n",
    "- The dimensionality of the vectors. Typical choices include a few dozen to several hundred.\n",
    "- The width of the sliding window, in tokens. Five is a common default choice, but narrower and wider windows are possible.\n",
    "- The number of training epochs.\n",
    "\n",
    "For using word2vec in Python, [gensim](https://rare-technologies.com/deep-learning-with-word2vec-and-gensim/) comes to the rescue again! It offers a [highly-optimized](https://rare-technologies.com/word2vec-in-python-part-two-optimizing/), [parallelized](https://rare-technologies.com/parallelizing-word2vec-in-python/) implementation of the word2vec algorithm with its [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our word2vec model using the normalized sentences with our phrase models applied. We'll use 100-dimensional vectors, and set up our training process to run for twelve epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the word2vec model yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    # initiate the model and perform the first epoch of training\n",
    "    food2vec = Word2Vec(trigram_sentences, size=100, window=5,\n",
    "                        min_count=20, sg=1, workers=4)\n",
    "    \n",
    "    food2vec.save(word2vec_filepath)\n",
    "\n",
    "    # perform another 11 epochs of training\n",
    "    for i in range(1,12):\n",
    "\n",
    "        food2vec.train(trigram_sentences)\n",
    "        food2vec.save(word2vec_filepath)\n",
    "        \n",
    "# load the finished model from disk\n",
    "food2vec = Word2Vec.load(word2vec_filepath)\n",
    "food2vec.init_sims()\n",
    "\n",
    "print u'{} training epochs so far.'.format(food2vec.train_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my four-core machine, each epoch over all the text in the ~1 million Yelp reviews takes about 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print u'{:,} terms in the food2vec vocabulary.'.format(len(food2vec.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the word vectors our model has learned. We'll create a pandas DataFrame with the terms as the row labels, and the 100 dimensions of the word vector model as the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a list of the terms, integer indices,\n",
    "# and term counts from the food2vec model vocabulary\n",
    "ordered_vocab = [(term, voc.index, voc.count)\n",
    "                 for term, voc in food2vec.vocab.iteritems()]\n",
    "\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda (term, index, count): -count)\n",
    "\n",
    "# unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# create a DataFrame with the food2vec vectors as data,\n",
    "# and the terms as row labels\n",
    "word_vectors = pd.DataFrame(food2vec.syn0norm[term_indices, :],\n",
    "                            index=ordered_terms)\n",
    "\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy wall of numbers! This DataFrame has 50,835 rows &mdash; one for each term in the vocabulary &mdash; and 100 colums. Our model has learned a quantitative vector representation for each term, as expected.\n",
    "\n",
    "Put another way, our model has \"embedded\" the terms into a 100-dimensional vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So... what can we do with all these numbers?\n",
    "The first thing we can use them for is to simply look up related words and phrases for a given term of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_related_terms(token, topn=10):\n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token\n",
    "    and print them as a formatted list\n",
    "    \"\"\"\n",
    "\n",
    "    for word, similarity in food2vec.most_similar(positive=[token], topn=topn):\n",
    "\n",
    "        print u'{:20} {}'.format(word, round(similarity, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
